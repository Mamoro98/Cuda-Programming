{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamoro98/Cuda-Programming/blob/main/omer_GPU_Practical4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Omer Kamal Ali Ebead</h1>"
      ],
      "metadata": {
        "id": "XKWltnE7DCZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs**\n",
        "\n",
        "# **Practical 4**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify the details of the GPU which is available to you, and upload the usual two header files."
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "08875df3-8801-47e3-a125-b0afccfdbb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jun  8 06:57:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "23b0fc14-c453-40fc-cb92-ce5980a84be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-08 06:57:34--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34238 (33K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "helper_cuda.h       100%[===================>]  33.44K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-08 06:57:35 (246 KB/s) - ‘helper_cuda.h’ saved [34238/34238]\n",
            "\n",
            "--2024-06-08 06:57:35--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23960 (23K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  23.40K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-06-08 06:57:36 (172 KB/s) - ‘helper_string.h’ saved [23960/23960]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file reduction.cu which includes within it a reference C++ routine against which the CUDA results are compared."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[tid];\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "\n",
        "    if (tid==0) g_odata[0] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "0f3530e9-2203-4d74-b54b-c8cb7e0f01e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.  Note that the compilation links in the CUDA random number generation library cuRAND.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "0a912c66-0b36-4ed3-c0ca-7b9572c79a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "6dc95d0b-84eb-4cfc-977d-2239255ec61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reduction error = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the initial Practical 4 exercises. Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For the first exercise, it may be useful to know that the following line of code will round up the input n to the nearest power of 2, so then dividing it by 2 gives the largest power of 2 less than n.\n",
        "\n",
        "`for (m=1; m<n; m=2*m) {} `\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should again add your name to the title of the notebook (as in \"Practical 4 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n"
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "tLhEaicpji2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}