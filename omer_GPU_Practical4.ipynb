{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "i1JlUA_e44zk",
        "Zf7mLJPjLU9_",
        "ynryJFtOQzrF",
        "Q9My4q9FRE-F",
        "efGFX3LlzwAV",
        "Iz_tllz9Z8-4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamoro98/Cuda-Programming/blob/main/omer_GPU_Practical4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Omer Kamal Ali Ebead</h1>"
      ],
      "metadata": {
        "id": "XKWltnE7DCZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs**\n",
        "\n",
        "# **Practical 4**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify the details of the GPU which is available to you, and upload the usual two header files."
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "c071f011-5fcb-4d4a-e74b-a9c9baf890fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 17 14:53:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "c1ce2d2c-62dc-47a3-a7f6-b0bfb170a20d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-17 14:53:41--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27832 (27K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "\rhelper_cuda.h         0%[                    ]       0  --.-KB/s               \rhelper_cuda.h       100%[===================>]  27.18K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-17 14:53:41 (2.10 MB/s) - ‘helper_cuda.h’ saved [27832/27832]\n",
            "\n",
            "--2025-02-17 14:53:41--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14875 (15K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  14.53K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-17 14:53:41 (1.13 MB/s) - ‘helper_string.h’ saved [14875/14875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file reduction.cu which includes within it a reference C++ routine against which the CUDA results are compared."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "Zf7mLJPjLU9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[tid];\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "    if (tid==0) g_odata[0] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "  printf(\"reduction result = %f\\n\",h_data[0]);\n",
        "  printf(\"reference result = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "238861ac-e020-4edd-8292-7e53dc088bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.  Note that the compilation links in the CUDA random number generation library cuRAND.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "b3cd425d-e2ac-4389-e6c2-d2a3e9804d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "57035735-a9fc-4a58-adfd-8cf85ade75fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n",
            "reduction result = 2351.000000\n",
            "reference result = 2351.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**explain why this shows the result is correct, and how the code has performed the required check.**"
      ],
      "metadata": {
        "id": "v6gMOqORD1cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The program computes the sum in two ways:\n",
        "\n",
        "1. **Using the CPU (`reduction_gold()`):**  \n",
        "   - This function computes the sum sequentially using a simple for-loop.\n",
        "\n",
        "2. **Using the GPU (`reduction()`):**  \n",
        "   - The CUDA kernel performs **parallel reduction** using **shared memory**.\n",
        "\n",
        "At the end of execution:\n",
        "- The **GPU result is copied back to the CPU**.\n",
        "- The **computed sum is compared** to the **CPU reference sum**.\n",
        "- If the **difference is zero**, the implementation is correct.\n",
        "\n",
        "\n",
        "### **1 Compute the CPU Reference Sum**\n",
        "The CPU computes the **expected correct sum** using:\n",
        "$$\n",
        "\\text{sum} = \\sum_{i=0}^{N-1} \\text{idata}[i]\n",
        "$$\n",
        "\n",
        "### **2 Copy the GPU Result to the CPU**\n",
        "$$\n",
        "\\text{cudaMemcpy}(h\\_data, d\\_odata, \\text{sizeof(float)}, \\text{cudaMemcpyDeviceToHost})\n",
        "$$\n",
        "This retrieves the **GPU result (`d_odata[0]`)** after reduction.\n",
        "\n",
        "### **3 Compare GPU and CPU Results**\n",
        "The program prints:\n",
        "$$\n",
        "\\text{reduction error} = h\\_data[0] - \\text{sum}\n",
        "$$\n",
        "- If the **difference is zero (0.0f)**, the GPU result is correct.\n"
      ],
      "metadata": {
        "id": "QKShHth8J3jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "Q1mSDA1wD8iZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Round up code"
      ],
      "metadata": {
        "id": "ynryJFtOQzrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int m1, m2, m3;\n",
        "\n",
        "  for (int n=2; n<1029; n++){\n",
        "\n",
        "    for (m1=1; m1<n; m1=2*m1) {}\n",
        "\n",
        "    m2 = n-1;\n",
        "    m2 = m2 | (m2>>1);\n",
        "    m2 = m2 | (m2>>2);\n",
        "    // m2 = m2 | (m2>>4);\n",
        "    // m2 = m2 | (m2>>8);  // this handles up to 16-bit integers\n",
        "    // m2 = m2 | (m2>>16); // needed to go up to 32-bit integers\n",
        "    m2 = m2 + 1;\n",
        "\n",
        "    // in line below need to rename to  __clz() in CUDA; see\n",
        "    // 1.10 in https://docs.nvidia.com/cuda/cuda-math-api/\n",
        "    m3 = 1 << (64 - __builtin_clz(n-1));  //  needs n>1\n",
        "\n",
        "    printf(\"n, m1, m2, m3 = %2d, %2d, %2d, %2d \\n\",n,m1,m2,m3);\n",
        "  }\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tH1bLSQ2Lqs6",
        "outputId": "e36019e8-cb95-450c-9728-112be7872751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yXI2vaJLqwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4440a5-d907-41e7-834c-a06c088a4a88",
        "id": "8UG2iiEPMz2i"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d847926-c903-412a-ca93-6c623b89709e",
        "collapsed": true,
        "id": "ImG4DBSOMz2k"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n, m1, m2, m3 =  2,  2,  2,  2 \n",
            "n, m1, m2, m3 =  3,  4,  4,  4 \n",
            "n, m1, m2, m3 =  4,  4,  4,  4 \n",
            "n, m1, m2, m3 =  5,  8,  8,  8 \n",
            "n, m1, m2, m3 =  6,  8,  8,  8 \n",
            "n, m1, m2, m3 =  7,  8,  8,  8 \n",
            "n, m1, m2, m3 =  8,  8,  8,  8 \n",
            "n, m1, m2, m3 =  9, 16, 16, 16 \n",
            "n, m1, m2, m3 = 10, 16, 16, 16 \n",
            "n, m1, m2, m3 = 11, 16, 16, 16 \n",
            "n, m1, m2, m3 = 12, 16, 16, 16 \n",
            "n, m1, m2, m3 = 13, 16, 16, 16 \n",
            "n, m1, m2, m3 = 14, 16, 16, 16 \n",
            "n, m1, m2, m3 = 15, 16, 16, 16 \n",
            "n, m1, m2, m3 = 16, 16, 16, 16 \n",
            "n, m1, m2, m3 = 17, 32, 31, 32 \n",
            "n, m1, m2, m3 = 18, 32, 32, 32 \n",
            "n, m1, m2, m3 = 19, 32, 32, 32 \n",
            "n, m1, m2, m3 = 20, 32, 32, 32 \n",
            "n, m1, m2, m3 = 21, 32, 32, 32 \n",
            "n, m1, m2, m3 = 22, 32, 32, 32 \n",
            "n, m1, m2, m3 = 23, 32, 32, 32 \n",
            "n, m1, m2, m3 = 24, 32, 32, 32 \n",
            "n, m1, m2, m3 = 25, 32, 32, 32 \n",
            "n, m1, m2, m3 = 26, 32, 32, 32 \n",
            "n, m1, m2, m3 = 27, 32, 32, 32 \n",
            "n, m1, m2, m3 = 28, 32, 32, 32 \n",
            "n, m1, m2, m3 = 29, 32, 32, 32 \n",
            "n, m1, m2, m3 = 30, 32, 32, 32 \n",
            "n, m1, m2, m3 = 31, 32, 32, 32 \n",
            "n, m1, m2, m3 = 32, 32, 32, 32 \n",
            "n, m1, m2, m3 = 33, 64, 61, 64 \n",
            "n, m1, m2, m3 = 34, 64, 62, 64 \n",
            "n, m1, m2, m3 = 35, 64, 64, 64 \n",
            "n, m1, m2, m3 = 36, 64, 64, 64 \n",
            "n, m1, m2, m3 = 37, 64, 64, 64 \n",
            "n, m1, m2, m3 = 38, 64, 64, 64 \n",
            "n, m1, m2, m3 = 39, 64, 64, 64 \n",
            "n, m1, m2, m3 = 40, 64, 64, 64 \n",
            "n, m1, m2, m3 = 41, 64, 64, 64 \n",
            "n, m1, m2, m3 = 42, 64, 64, 64 \n",
            "n, m1, m2, m3 = 43, 64, 64, 64 \n",
            "n, m1, m2, m3 = 44, 64, 64, 64 \n",
            "n, m1, m2, m3 = 45, 64, 64, 64 \n",
            "n, m1, m2, m3 = 46, 64, 64, 64 \n",
            "n, m1, m2, m3 = 47, 64, 64, 64 \n",
            "n, m1, m2, m3 = 48, 64, 64, 64 \n",
            "n, m1, m2, m3 = 49, 64, 63, 64 \n",
            "n, m1, m2, m3 = 50, 64, 64, 64 \n",
            "n, m1, m2, m3 = 51, 64, 64, 64 \n",
            "n, m1, m2, m3 = 52, 64, 64, 64 \n",
            "n, m1, m2, m3 = 53, 64, 64, 64 \n",
            "n, m1, m2, m3 = 54, 64, 64, 64 \n",
            "n, m1, m2, m3 = 55, 64, 64, 64 \n",
            "n, m1, m2, m3 = 56, 64, 64, 64 \n",
            "n, m1, m2, m3 = 57, 64, 64, 64 \n",
            "n, m1, m2, m3 = 58, 64, 64, 64 \n",
            "n, m1, m2, m3 = 59, 64, 64, 64 \n",
            "n, m1, m2, m3 = 60, 64, 64, 64 \n",
            "n, m1, m2, m3 = 61, 64, 64, 64 \n",
            "n, m1, m2, m3 = 62, 64, 64, 64 \n",
            "n, m1, m2, m3 = 63, 64, 64, 64 \n",
            "n, m1, m2, m3 = 64, 64, 64, 64 \n",
            "n, m1, m2, m3 = 65, 128, 121, 128 \n",
            "n, m1, m2, m3 = 66, 128, 122, 128 \n",
            "n, m1, m2, m3 = 67, 128, 124, 128 \n",
            "n, m1, m2, m3 = 68, 128, 124, 128 \n",
            "n, m1, m2, m3 = 69, 128, 128, 128 \n",
            "n, m1, m2, m3 = 70, 128, 128, 128 \n",
            "n, m1, m2, m3 = 71, 128, 128, 128 \n",
            "n, m1, m2, m3 = 72, 128, 128, 128 \n",
            "n, m1, m2, m3 = 73, 128, 128, 128 \n",
            "n, m1, m2, m3 = 74, 128, 128, 128 \n",
            "n, m1, m2, m3 = 75, 128, 128, 128 \n",
            "n, m1, m2, m3 = 76, 128, 128, 128 \n",
            "n, m1, m2, m3 = 77, 128, 128, 128 \n",
            "n, m1, m2, m3 = 78, 128, 128, 128 \n",
            "n, m1, m2, m3 = 79, 128, 128, 128 \n",
            "n, m1, m2, m3 = 80, 128, 128, 128 \n",
            "n, m1, m2, m3 = 81, 128, 127, 128 \n",
            "n, m1, m2, m3 = 82, 128, 128, 128 \n",
            "n, m1, m2, m3 = 83, 128, 128, 128 \n",
            "n, m1, m2, m3 = 84, 128, 128, 128 \n",
            "n, m1, m2, m3 = 85, 128, 128, 128 \n",
            "n, m1, m2, m3 = 86, 128, 128, 128 \n",
            "n, m1, m2, m3 = 87, 128, 128, 128 \n",
            "n, m1, m2, m3 = 88, 128, 128, 128 \n",
            "n, m1, m2, m3 = 89, 128, 128, 128 \n",
            "n, m1, m2, m3 = 90, 128, 128, 128 \n",
            "n, m1, m2, m3 = 91, 128, 128, 128 \n",
            "n, m1, m2, m3 = 92, 128, 128, 128 \n",
            "n, m1, m2, m3 = 93, 128, 128, 128 \n",
            "n, m1, m2, m3 = 94, 128, 128, 128 \n",
            "n, m1, m2, m3 = 95, 128, 128, 128 \n",
            "n, m1, m2, m3 = 96, 128, 128, 128 \n",
            "n, m1, m2, m3 = 97, 128, 125, 128 \n",
            "n, m1, m2, m3 = 98, 128, 126, 128 \n",
            "n, m1, m2, m3 = 99, 128, 128, 128 \n",
            "n, m1, m2, m3 = 100, 128, 128, 128 \n",
            "n, m1, m2, m3 = 101, 128, 128, 128 \n",
            "n, m1, m2, m3 = 102, 128, 128, 128 \n",
            "n, m1, m2, m3 = 103, 128, 128, 128 \n",
            "n, m1, m2, m3 = 104, 128, 128, 128 \n",
            "n, m1, m2, m3 = 105, 128, 128, 128 \n",
            "n, m1, m2, m3 = 106, 128, 128, 128 \n",
            "n, m1, m2, m3 = 107, 128, 128, 128 \n",
            "n, m1, m2, m3 = 108, 128, 128, 128 \n",
            "n, m1, m2, m3 = 109, 128, 128, 128 \n",
            "n, m1, m2, m3 = 110, 128, 128, 128 \n",
            "n, m1, m2, m3 = 111, 128, 128, 128 \n",
            "n, m1, m2, m3 = 112, 128, 128, 128 \n",
            "n, m1, m2, m3 = 113, 128, 127, 128 \n",
            "n, m1, m2, m3 = 114, 128, 128, 128 \n",
            "n, m1, m2, m3 = 115, 128, 128, 128 \n",
            "n, m1, m2, m3 = 116, 128, 128, 128 \n",
            "n, m1, m2, m3 = 117, 128, 128, 128 \n",
            "n, m1, m2, m3 = 118, 128, 128, 128 \n",
            "n, m1, m2, m3 = 119, 128, 128, 128 \n",
            "n, m1, m2, m3 = 120, 128, 128, 128 \n",
            "n, m1, m2, m3 = 121, 128, 128, 128 \n",
            "n, m1, m2, m3 = 122, 128, 128, 128 \n",
            "n, m1, m2, m3 = 123, 128, 128, 128 \n",
            "n, m1, m2, m3 = 124, 128, 128, 128 \n",
            "n, m1, m2, m3 = 125, 128, 128, 128 \n",
            "n, m1, m2, m3 = 126, 128, 128, 128 \n",
            "n, m1, m2, m3 = 127, 128, 128, 128 \n",
            "n, m1, m2, m3 = 128, 128, 128, 128 \n",
            "n, m1, m2, m3 = 129, 256, 241, 256 \n",
            "n, m1, m2, m3 = 130, 256, 242, 256 \n",
            "n, m1, m2, m3 = 131, 256, 244, 256 \n",
            "n, m1, m2, m3 = 132, 256, 244, 256 \n",
            "n, m1, m2, m3 = 133, 256, 248, 256 \n",
            "n, m1, m2, m3 = 134, 256, 248, 256 \n",
            "n, m1, m2, m3 = 135, 256, 248, 256 \n",
            "n, m1, m2, m3 = 136, 256, 248, 256 \n",
            "n, m1, m2, m3 = 137, 256, 256, 256 \n",
            "n, m1, m2, m3 = 138, 256, 256, 256 \n",
            "n, m1, m2, m3 = 139, 256, 256, 256 \n",
            "n, m1, m2, m3 = 140, 256, 256, 256 \n",
            "n, m1, m2, m3 = 141, 256, 256, 256 \n",
            "n, m1, m2, m3 = 142, 256, 256, 256 \n",
            "n, m1, m2, m3 = 143, 256, 256, 256 \n",
            "n, m1, m2, m3 = 144, 256, 256, 256 \n",
            "n, m1, m2, m3 = 145, 256, 255, 256 \n",
            "n, m1, m2, m3 = 146, 256, 256, 256 \n",
            "n, m1, m2, m3 = 147, 256, 256, 256 \n",
            "n, m1, m2, m3 = 148, 256, 256, 256 \n",
            "n, m1, m2, m3 = 149, 256, 256, 256 \n",
            "n, m1, m2, m3 = 150, 256, 256, 256 \n",
            "n, m1, m2, m3 = 151, 256, 256, 256 \n",
            "n, m1, m2, m3 = 152, 256, 256, 256 \n",
            "n, m1, m2, m3 = 153, 256, 256, 256 \n",
            "n, m1, m2, m3 = 154, 256, 256, 256 \n",
            "n, m1, m2, m3 = 155, 256, 256, 256 \n",
            "n, m1, m2, m3 = 156, 256, 256, 256 \n",
            "n, m1, m2, m3 = 157, 256, 256, 256 \n",
            "n, m1, m2, m3 = 158, 256, 256, 256 \n",
            "n, m1, m2, m3 = 159, 256, 256, 256 \n",
            "n, m1, m2, m3 = 160, 256, 256, 256 \n",
            "n, m1, m2, m3 = 161, 256, 253, 256 \n",
            "n, m1, m2, m3 = 162, 256, 254, 256 \n",
            "n, m1, m2, m3 = 163, 256, 256, 256 \n",
            "n, m1, m2, m3 = 164, 256, 256, 256 \n",
            "n, m1, m2, m3 = 165, 256, 256, 256 \n",
            "n, m1, m2, m3 = 166, 256, 256, 256 \n",
            "n, m1, m2, m3 = 167, 256, 256, 256 \n",
            "n, m1, m2, m3 = 168, 256, 256, 256 \n",
            "n, m1, m2, m3 = 169, 256, 256, 256 \n",
            "n, m1, m2, m3 = 170, 256, 256, 256 \n",
            "n, m1, m2, m3 = 171, 256, 256, 256 \n",
            "n, m1, m2, m3 = 172, 256, 256, 256 \n",
            "n, m1, m2, m3 = 173, 256, 256, 256 \n",
            "n, m1, m2, m3 = 174, 256, 256, 256 \n",
            "n, m1, m2, m3 = 175, 256, 256, 256 \n",
            "n, m1, m2, m3 = 176, 256, 256, 256 \n",
            "n, m1, m2, m3 = 177, 256, 255, 256 \n",
            "n, m1, m2, m3 = 178, 256, 256, 256 \n",
            "n, m1, m2, m3 = 179, 256, 256, 256 \n",
            "n, m1, m2, m3 = 180, 256, 256, 256 \n",
            "n, m1, m2, m3 = 181, 256, 256, 256 \n",
            "n, m1, m2, m3 = 182, 256, 256, 256 \n",
            "n, m1, m2, m3 = 183, 256, 256, 256 \n",
            "n, m1, m2, m3 = 184, 256, 256, 256 \n",
            "n, m1, m2, m3 = 185, 256, 256, 256 \n",
            "n, m1, m2, m3 = 186, 256, 256, 256 \n",
            "n, m1, m2, m3 = 187, 256, 256, 256 \n",
            "n, m1, m2, m3 = 188, 256, 256, 256 \n",
            "n, m1, m2, m3 = 189, 256, 256, 256 \n",
            "n, m1, m2, m3 = 190, 256, 256, 256 \n",
            "n, m1, m2, m3 = 191, 256, 256, 256 \n",
            "n, m1, m2, m3 = 192, 256, 256, 256 \n",
            "n, m1, m2, m3 = 193, 256, 249, 256 \n",
            "n, m1, m2, m3 = 194, 256, 250, 256 \n",
            "n, m1, m2, m3 = 195, 256, 252, 256 \n",
            "n, m1, m2, m3 = 196, 256, 252, 256 \n",
            "n, m1, m2, m3 = 197, 256, 256, 256 \n",
            "n, m1, m2, m3 = 198, 256, 256, 256 \n",
            "n, m1, m2, m3 = 199, 256, 256, 256 \n",
            "n, m1, m2, m3 = 200, 256, 256, 256 \n",
            "n, m1, m2, m3 = 201, 256, 256, 256 \n",
            "n, m1, m2, m3 = 202, 256, 256, 256 \n",
            "n, m1, m2, m3 = 203, 256, 256, 256 \n",
            "n, m1, m2, m3 = 204, 256, 256, 256 \n",
            "n, m1, m2, m3 = 205, 256, 256, 256 \n",
            "n, m1, m2, m3 = 206, 256, 256, 256 \n",
            "n, m1, m2, m3 = 207, 256, 256, 256 \n",
            "n, m1, m2, m3 = 208, 256, 256, 256 \n",
            "n, m1, m2, m3 = 209, 256, 255, 256 \n",
            "n, m1, m2, m3 = 210, 256, 256, 256 \n",
            "n, m1, m2, m3 = 211, 256, 256, 256 \n",
            "n, m1, m2, m3 = 212, 256, 256, 256 \n",
            "n, m1, m2, m3 = 213, 256, 256, 256 \n",
            "n, m1, m2, m3 = 214, 256, 256, 256 \n",
            "n, m1, m2, m3 = 215, 256, 256, 256 \n",
            "n, m1, m2, m3 = 216, 256, 256, 256 \n",
            "n, m1, m2, m3 = 217, 256, 256, 256 \n",
            "n, m1, m2, m3 = 218, 256, 256, 256 \n",
            "n, m1, m2, m3 = 219, 256, 256, 256 \n",
            "n, m1, m2, m3 = 220, 256, 256, 256 \n",
            "n, m1, m2, m3 = 221, 256, 256, 256 \n",
            "n, m1, m2, m3 = 222, 256, 256, 256 \n",
            "n, m1, m2, m3 = 223, 256, 256, 256 \n",
            "n, m1, m2, m3 = 224, 256, 256, 256 \n",
            "n, m1, m2, m3 = 225, 256, 253, 256 \n",
            "n, m1, m2, m3 = 226, 256, 254, 256 \n",
            "n, m1, m2, m3 = 227, 256, 256, 256 \n",
            "n, m1, m2, m3 = 228, 256, 256, 256 \n",
            "n, m1, m2, m3 = 229, 256, 256, 256 \n",
            "n, m1, m2, m3 = 230, 256, 256, 256 \n",
            "n, m1, m2, m3 = 231, 256, 256, 256 \n",
            "n, m1, m2, m3 = 232, 256, 256, 256 \n",
            "n, m1, m2, m3 = 233, 256, 256, 256 \n",
            "n, m1, m2, m3 = 234, 256, 256, 256 \n",
            "n, m1, m2, m3 = 235, 256, 256, 256 \n",
            "n, m1, m2, m3 = 236, 256, 256, 256 \n",
            "n, m1, m2, m3 = 237, 256, 256, 256 \n",
            "n, m1, m2, m3 = 238, 256, 256, 256 \n",
            "n, m1, m2, m3 = 239, 256, 256, 256 \n",
            "n, m1, m2, m3 = 240, 256, 256, 256 \n",
            "n, m1, m2, m3 = 241, 256, 255, 256 \n",
            "n, m1, m2, m3 = 242, 256, 256, 256 \n",
            "n, m1, m2, m3 = 243, 256, 256, 256 \n",
            "n, m1, m2, m3 = 244, 256, 256, 256 \n",
            "n, m1, m2, m3 = 245, 256, 256, 256 \n",
            "n, m1, m2, m3 = 246, 256, 256, 256 \n",
            "n, m1, m2, m3 = 247, 256, 256, 256 \n",
            "n, m1, m2, m3 = 248, 256, 256, 256 \n",
            "n, m1, m2, m3 = 249, 256, 256, 256 \n",
            "n, m1, m2, m3 = 250, 256, 256, 256 \n",
            "n, m1, m2, m3 = 251, 256, 256, 256 \n",
            "n, m1, m2, m3 = 252, 256, 256, 256 \n",
            "n, m1, m2, m3 = 253, 256, 256, 256 \n",
            "n, m1, m2, m3 = 254, 256, 256, 256 \n",
            "n, m1, m2, m3 = 255, 256, 256, 256 \n",
            "n, m1, m2, m3 = 256, 256, 256, 256 \n",
            "n, m1, m2, m3 = 257, 512, 481, 512 \n",
            "n, m1, m2, m3 = 258, 512, 482, 512 \n",
            "n, m1, m2, m3 = 259, 512, 484, 512 \n",
            "n, m1, m2, m3 = 260, 512, 484, 512 \n",
            "n, m1, m2, m3 = 261, 512, 488, 512 \n",
            "n, m1, m2, m3 = 262, 512, 488, 512 \n",
            "n, m1, m2, m3 = 263, 512, 488, 512 \n",
            "n, m1, m2, m3 = 264, 512, 488, 512 \n",
            "n, m1, m2, m3 = 265, 512, 496, 512 \n",
            "n, m1, m2, m3 = 266, 512, 496, 512 \n",
            "n, m1, m2, m3 = 267, 512, 496, 512 \n",
            "n, m1, m2, m3 = 268, 512, 496, 512 \n",
            "n, m1, m2, m3 = 269, 512, 496, 512 \n",
            "n, m1, m2, m3 = 270, 512, 496, 512 \n",
            "n, m1, m2, m3 = 271, 512, 496, 512 \n",
            "n, m1, m2, m3 = 272, 512, 496, 512 \n",
            "n, m1, m2, m3 = 273, 512, 511, 512 \n",
            "n, m1, m2, m3 = 274, 512, 512, 512 \n",
            "n, m1, m2, m3 = 275, 512, 512, 512 \n",
            "n, m1, m2, m3 = 276, 512, 512, 512 \n",
            "n, m1, m2, m3 = 277, 512, 512, 512 \n",
            "n, m1, m2, m3 = 278, 512, 512, 512 \n",
            "n, m1, m2, m3 = 279, 512, 512, 512 \n",
            "n, m1, m2, m3 = 280, 512, 512, 512 \n",
            "n, m1, m2, m3 = 281, 512, 512, 512 \n",
            "n, m1, m2, m3 = 282, 512, 512, 512 \n",
            "n, m1, m2, m3 = 283, 512, 512, 512 \n",
            "n, m1, m2, m3 = 284, 512, 512, 512 \n",
            "n, m1, m2, m3 = 285, 512, 512, 512 \n",
            "n, m1, m2, m3 = 286, 512, 512, 512 \n",
            "n, m1, m2, m3 = 287, 512, 512, 512 \n",
            "n, m1, m2, m3 = 288, 512, 512, 512 \n",
            "n, m1, m2, m3 = 289, 512, 509, 512 \n",
            "n, m1, m2, m3 = 290, 512, 510, 512 \n",
            "n, m1, m2, m3 = 291, 512, 512, 512 \n",
            "n, m1, m2, m3 = 292, 512, 512, 512 \n",
            "n, m1, m2, m3 = 293, 512, 512, 512 \n",
            "n, m1, m2, m3 = 294, 512, 512, 512 \n",
            "n, m1, m2, m3 = 295, 512, 512, 512 \n",
            "n, m1, m2, m3 = 296, 512, 512, 512 \n",
            "n, m1, m2, m3 = 297, 512, 512, 512 \n",
            "n, m1, m2, m3 = 298, 512, 512, 512 \n",
            "n, m1, m2, m3 = 299, 512, 512, 512 \n",
            "n, m1, m2, m3 = 300, 512, 512, 512 \n",
            "n, m1, m2, m3 = 301, 512, 512, 512 \n",
            "n, m1, m2, m3 = 302, 512, 512, 512 \n",
            "n, m1, m2, m3 = 303, 512, 512, 512 \n",
            "n, m1, m2, m3 = 304, 512, 512, 512 \n",
            "n, m1, m2, m3 = 305, 512, 511, 512 \n",
            "n, m1, m2, m3 = 306, 512, 512, 512 \n",
            "n, m1, m2, m3 = 307, 512, 512, 512 \n",
            "n, m1, m2, m3 = 308, 512, 512, 512 \n",
            "n, m1, m2, m3 = 309, 512, 512, 512 \n",
            "n, m1, m2, m3 = 310, 512, 512, 512 \n",
            "n, m1, m2, m3 = 311, 512, 512, 512 \n",
            "n, m1, m2, m3 = 312, 512, 512, 512 \n",
            "n, m1, m2, m3 = 313, 512, 512, 512 \n",
            "n, m1, m2, m3 = 314, 512, 512, 512 \n",
            "n, m1, m2, m3 = 315, 512, 512, 512 \n",
            "n, m1, m2, m3 = 316, 512, 512, 512 \n",
            "n, m1, m2, m3 = 317, 512, 512, 512 \n",
            "n, m1, m2, m3 = 318, 512, 512, 512 \n",
            "n, m1, m2, m3 = 319, 512, 512, 512 \n",
            "n, m1, m2, m3 = 320, 512, 512, 512 \n",
            "n, m1, m2, m3 = 321, 512, 505, 512 \n",
            "n, m1, m2, m3 = 322, 512, 506, 512 \n",
            "n, m1, m2, m3 = 323, 512, 508, 512 \n",
            "n, m1, m2, m3 = 324, 512, 508, 512 \n",
            "n, m1, m2, m3 = 325, 512, 512, 512 \n",
            "n, m1, m2, m3 = 326, 512, 512, 512 \n",
            "n, m1, m2, m3 = 327, 512, 512, 512 \n",
            "n, m1, m2, m3 = 328, 512, 512, 512 \n",
            "n, m1, m2, m3 = 329, 512, 512, 512 \n",
            "n, m1, m2, m3 = 330, 512, 512, 512 \n",
            "n, m1, m2, m3 = 331, 512, 512, 512 \n",
            "n, m1, m2, m3 = 332, 512, 512, 512 \n",
            "n, m1, m2, m3 = 333, 512, 512, 512 \n",
            "n, m1, m2, m3 = 334, 512, 512, 512 \n",
            "n, m1, m2, m3 = 335, 512, 512, 512 \n",
            "n, m1, m2, m3 = 336, 512, 512, 512 \n",
            "n, m1, m2, m3 = 337, 512, 511, 512 \n",
            "n, m1, m2, m3 = 338, 512, 512, 512 \n",
            "n, m1, m2, m3 = 339, 512, 512, 512 \n",
            "n, m1, m2, m3 = 340, 512, 512, 512 \n",
            "n, m1, m2, m3 = 341, 512, 512, 512 \n",
            "n, m1, m2, m3 = 342, 512, 512, 512 \n",
            "n, m1, m2, m3 = 343, 512, 512, 512 \n",
            "n, m1, m2, m3 = 344, 512, 512, 512 \n",
            "n, m1, m2, m3 = 345, 512, 512, 512 \n",
            "n, m1, m2, m3 = 346, 512, 512, 512 \n",
            "n, m1, m2, m3 = 347, 512, 512, 512 \n",
            "n, m1, m2, m3 = 348, 512, 512, 512 \n",
            "n, m1, m2, m3 = 349, 512, 512, 512 \n",
            "n, m1, m2, m3 = 350, 512, 512, 512 \n",
            "n, m1, m2, m3 = 351, 512, 512, 512 \n",
            "n, m1, m2, m3 = 352, 512, 512, 512 \n",
            "n, m1, m2, m3 = 353, 512, 509, 512 \n",
            "n, m1, m2, m3 = 354, 512, 510, 512 \n",
            "n, m1, m2, m3 = 355, 512, 512, 512 \n",
            "n, m1, m2, m3 = 356, 512, 512, 512 \n",
            "n, m1, m2, m3 = 357, 512, 512, 512 \n",
            "n, m1, m2, m3 = 358, 512, 512, 512 \n",
            "n, m1, m2, m3 = 359, 512, 512, 512 \n",
            "n, m1, m2, m3 = 360, 512, 512, 512 \n",
            "n, m1, m2, m3 = 361, 512, 512, 512 \n",
            "n, m1, m2, m3 = 362, 512, 512, 512 \n",
            "n, m1, m2, m3 = 363, 512, 512, 512 \n",
            "n, m1, m2, m3 = 364, 512, 512, 512 \n",
            "n, m1, m2, m3 = 365, 512, 512, 512 \n",
            "n, m1, m2, m3 = 366, 512, 512, 512 \n",
            "n, m1, m2, m3 = 367, 512, 512, 512 \n",
            "n, m1, m2, m3 = 368, 512, 512, 512 \n",
            "n, m1, m2, m3 = 369, 512, 511, 512 \n",
            "n, m1, m2, m3 = 370, 512, 512, 512 \n",
            "n, m1, m2, m3 = 371, 512, 512, 512 \n",
            "n, m1, m2, m3 = 372, 512, 512, 512 \n",
            "n, m1, m2, m3 = 373, 512, 512, 512 \n",
            "n, m1, m2, m3 = 374, 512, 512, 512 \n",
            "n, m1, m2, m3 = 375, 512, 512, 512 \n",
            "n, m1, m2, m3 = 376, 512, 512, 512 \n",
            "n, m1, m2, m3 = 377, 512, 512, 512 \n",
            "n, m1, m2, m3 = 378, 512, 512, 512 \n",
            "n, m1, m2, m3 = 379, 512, 512, 512 \n",
            "n, m1, m2, m3 = 380, 512, 512, 512 \n",
            "n, m1, m2, m3 = 381, 512, 512, 512 \n",
            "n, m1, m2, m3 = 382, 512, 512, 512 \n",
            "n, m1, m2, m3 = 383, 512, 512, 512 \n",
            "n, m1, m2, m3 = 384, 512, 512, 512 \n",
            "n, m1, m2, m3 = 385, 512, 497, 512 \n",
            "n, m1, m2, m3 = 386, 512, 498, 512 \n",
            "n, m1, m2, m3 = 387, 512, 500, 512 \n",
            "n, m1, m2, m3 = 388, 512, 500, 512 \n",
            "n, m1, m2, m3 = 389, 512, 504, 512 \n",
            "n, m1, m2, m3 = 390, 512, 504, 512 \n",
            "n, m1, m2, m3 = 391, 512, 504, 512 \n",
            "n, m1, m2, m3 = 392, 512, 504, 512 \n",
            "n, m1, m2, m3 = 393, 512, 512, 512 \n",
            "n, m1, m2, m3 = 394, 512, 512, 512 \n",
            "n, m1, m2, m3 = 395, 512, 512, 512 \n",
            "n, m1, m2, m3 = 396, 512, 512, 512 \n",
            "n, m1, m2, m3 = 397, 512, 512, 512 \n",
            "n, m1, m2, m3 = 398, 512, 512, 512 \n",
            "n, m1, m2, m3 = 399, 512, 512, 512 \n",
            "n, m1, m2, m3 = 400, 512, 512, 512 \n",
            "n, m1, m2, m3 = 401, 512, 511, 512 \n",
            "n, m1, m2, m3 = 402, 512, 512, 512 \n",
            "n, m1, m2, m3 = 403, 512, 512, 512 \n",
            "n, m1, m2, m3 = 404, 512, 512, 512 \n",
            "n, m1, m2, m3 = 405, 512, 512, 512 \n",
            "n, m1, m2, m3 = 406, 512, 512, 512 \n",
            "n, m1, m2, m3 = 407, 512, 512, 512 \n",
            "n, m1, m2, m3 = 408, 512, 512, 512 \n",
            "n, m1, m2, m3 = 409, 512, 512, 512 \n",
            "n, m1, m2, m3 = 410, 512, 512, 512 \n",
            "n, m1, m2, m3 = 411, 512, 512, 512 \n",
            "n, m1, m2, m3 = 412, 512, 512, 512 \n",
            "n, m1, m2, m3 = 413, 512, 512, 512 \n",
            "n, m1, m2, m3 = 414, 512, 512, 512 \n",
            "n, m1, m2, m3 = 415, 512, 512, 512 \n",
            "n, m1, m2, m3 = 416, 512, 512, 512 \n",
            "n, m1, m2, m3 = 417, 512, 509, 512 \n",
            "n, m1, m2, m3 = 418, 512, 510, 512 \n",
            "n, m1, m2, m3 = 419, 512, 512, 512 \n",
            "n, m1, m2, m3 = 420, 512, 512, 512 \n",
            "n, m1, m2, m3 = 421, 512, 512, 512 \n",
            "n, m1, m2, m3 = 422, 512, 512, 512 \n",
            "n, m1, m2, m3 = 423, 512, 512, 512 \n",
            "n, m1, m2, m3 = 424, 512, 512, 512 \n",
            "n, m1, m2, m3 = 425, 512, 512, 512 \n",
            "n, m1, m2, m3 = 426, 512, 512, 512 \n",
            "n, m1, m2, m3 = 427, 512, 512, 512 \n",
            "n, m1, m2, m3 = 428, 512, 512, 512 \n",
            "n, m1, m2, m3 = 429, 512, 512, 512 \n",
            "n, m1, m2, m3 = 430, 512, 512, 512 \n",
            "n, m1, m2, m3 = 431, 512, 512, 512 \n",
            "n, m1, m2, m3 = 432, 512, 512, 512 \n",
            "n, m1, m2, m3 = 433, 512, 511, 512 \n",
            "n, m1, m2, m3 = 434, 512, 512, 512 \n",
            "n, m1, m2, m3 = 435, 512, 512, 512 \n",
            "n, m1, m2, m3 = 436, 512, 512, 512 \n",
            "n, m1, m2, m3 = 437, 512, 512, 512 \n",
            "n, m1, m2, m3 = 438, 512, 512, 512 \n",
            "n, m1, m2, m3 = 439, 512, 512, 512 \n",
            "n, m1, m2, m3 = 440, 512, 512, 512 \n",
            "n, m1, m2, m3 = 441, 512, 512, 512 \n",
            "n, m1, m2, m3 = 442, 512, 512, 512 \n",
            "n, m1, m2, m3 = 443, 512, 512, 512 \n",
            "n, m1, m2, m3 = 444, 512, 512, 512 \n",
            "n, m1, m2, m3 = 445, 512, 512, 512 \n",
            "n, m1, m2, m3 = 446, 512, 512, 512 \n",
            "n, m1, m2, m3 = 447, 512, 512, 512 \n",
            "n, m1, m2, m3 = 448, 512, 512, 512 \n",
            "n, m1, m2, m3 = 449, 512, 505, 512 \n",
            "n, m1, m2, m3 = 450, 512, 506, 512 \n",
            "n, m1, m2, m3 = 451, 512, 508, 512 \n",
            "n, m1, m2, m3 = 452, 512, 508, 512 \n",
            "n, m1, m2, m3 = 453, 512, 512, 512 \n",
            "n, m1, m2, m3 = 454, 512, 512, 512 \n",
            "n, m1, m2, m3 = 455, 512, 512, 512 \n",
            "n, m1, m2, m3 = 456, 512, 512, 512 \n",
            "n, m1, m2, m3 = 457, 512, 512, 512 \n",
            "n, m1, m2, m3 = 458, 512, 512, 512 \n",
            "n, m1, m2, m3 = 459, 512, 512, 512 \n",
            "n, m1, m2, m3 = 460, 512, 512, 512 \n",
            "n, m1, m2, m3 = 461, 512, 512, 512 \n",
            "n, m1, m2, m3 = 462, 512, 512, 512 \n",
            "n, m1, m2, m3 = 463, 512, 512, 512 \n",
            "n, m1, m2, m3 = 464, 512, 512, 512 \n",
            "n, m1, m2, m3 = 465, 512, 511, 512 \n",
            "n, m1, m2, m3 = 466, 512, 512, 512 \n",
            "n, m1, m2, m3 = 467, 512, 512, 512 \n",
            "n, m1, m2, m3 = 468, 512, 512, 512 \n",
            "n, m1, m2, m3 = 469, 512, 512, 512 \n",
            "n, m1, m2, m3 = 470, 512, 512, 512 \n",
            "n, m1, m2, m3 = 471, 512, 512, 512 \n",
            "n, m1, m2, m3 = 472, 512, 512, 512 \n",
            "n, m1, m2, m3 = 473, 512, 512, 512 \n",
            "n, m1, m2, m3 = 474, 512, 512, 512 \n",
            "n, m1, m2, m3 = 475, 512, 512, 512 \n",
            "n, m1, m2, m3 = 476, 512, 512, 512 \n",
            "n, m1, m2, m3 = 477, 512, 512, 512 \n",
            "n, m1, m2, m3 = 478, 512, 512, 512 \n",
            "n, m1, m2, m3 = 479, 512, 512, 512 \n",
            "n, m1, m2, m3 = 480, 512, 512, 512 \n",
            "n, m1, m2, m3 = 481, 512, 509, 512 \n",
            "n, m1, m2, m3 = 482, 512, 510, 512 \n",
            "n, m1, m2, m3 = 483, 512, 512, 512 \n",
            "n, m1, m2, m3 = 484, 512, 512, 512 \n",
            "n, m1, m2, m3 = 485, 512, 512, 512 \n",
            "n, m1, m2, m3 = 486, 512, 512, 512 \n",
            "n, m1, m2, m3 = 487, 512, 512, 512 \n",
            "n, m1, m2, m3 = 488, 512, 512, 512 \n",
            "n, m1, m2, m3 = 489, 512, 512, 512 \n",
            "n, m1, m2, m3 = 490, 512, 512, 512 \n",
            "n, m1, m2, m3 = 491, 512, 512, 512 \n",
            "n, m1, m2, m3 = 492, 512, 512, 512 \n",
            "n, m1, m2, m3 = 493, 512, 512, 512 \n",
            "n, m1, m2, m3 = 494, 512, 512, 512 \n",
            "n, m1, m2, m3 = 495, 512, 512, 512 \n",
            "n, m1, m2, m3 = 496, 512, 512, 512 \n",
            "n, m1, m2, m3 = 497, 512, 511, 512 \n",
            "n, m1, m2, m3 = 498, 512, 512, 512 \n",
            "n, m1, m2, m3 = 499, 512, 512, 512 \n",
            "n, m1, m2, m3 = 500, 512, 512, 512 \n",
            "n, m1, m2, m3 = 501, 512, 512, 512 \n",
            "n, m1, m2, m3 = 502, 512, 512, 512 \n",
            "n, m1, m2, m3 = 503, 512, 512, 512 \n",
            "n, m1, m2, m3 = 504, 512, 512, 512 \n",
            "n, m1, m2, m3 = 505, 512, 512, 512 \n",
            "n, m1, m2, m3 = 506, 512, 512, 512 \n",
            "n, m1, m2, m3 = 507, 512, 512, 512 \n",
            "n, m1, m2, m3 = 508, 512, 512, 512 \n",
            "n, m1, m2, m3 = 509, 512, 512, 512 \n",
            "n, m1, m2, m3 = 510, 512, 512, 512 \n",
            "n, m1, m2, m3 = 511, 512, 512, 512 \n",
            "n, m1, m2, m3 = 512, 512, 512, 512 \n",
            "n, m1, m2, m3 = 513, 1024, 961, 1024 \n",
            "n, m1, m2, m3 = 514, 1024, 962, 1024 \n",
            "n, m1, m2, m3 = 515, 1024, 964, 1024 \n",
            "n, m1, m2, m3 = 516, 1024, 964, 1024 \n",
            "n, m1, m2, m3 = 517, 1024, 968, 1024 \n",
            "n, m1, m2, m3 = 518, 1024, 968, 1024 \n",
            "n, m1, m2, m3 = 519, 1024, 968, 1024 \n",
            "n, m1, m2, m3 = 520, 1024, 968, 1024 \n",
            "n, m1, m2, m3 = 521, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 522, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 523, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 524, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 525, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 526, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 527, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 528, 1024, 976, 1024 \n",
            "n, m1, m2, m3 = 529, 1024, 991, 1024 \n",
            "n, m1, m2, m3 = 530, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 531, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 532, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 533, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 534, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 535, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 536, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 537, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 538, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 539, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 540, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 541, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 542, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 543, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 544, 1024, 992, 1024 \n",
            "n, m1, m2, m3 = 545, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 546, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 547, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 548, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 549, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 550, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 551, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 552, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 553, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 554, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 555, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 556, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 557, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 558, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 559, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 560, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 561, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 562, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 563, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 564, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 565, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 566, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 567, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 568, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 569, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 570, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 571, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 572, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 573, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 574, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 575, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 576, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 577, 1024, 1017, 1024 \n",
            "n, m1, m2, m3 = 578, 1024, 1018, 1024 \n",
            "n, m1, m2, m3 = 579, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 580, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 581, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 582, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 583, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 584, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 585, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 586, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 587, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 588, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 589, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 590, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 591, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 592, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 593, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 594, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 595, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 596, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 597, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 598, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 599, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 600, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 601, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 602, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 603, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 604, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 605, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 606, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 607, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 608, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 609, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 610, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 611, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 612, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 613, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 614, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 615, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 616, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 617, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 618, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 619, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 620, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 621, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 622, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 623, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 624, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 625, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 626, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 627, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 628, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 629, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 630, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 631, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 632, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 633, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 634, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 635, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 636, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 637, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 638, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 639, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 640, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 641, 1024, 1009, 1024 \n",
            "n, m1, m2, m3 = 642, 1024, 1010, 1024 \n",
            "n, m1, m2, m3 = 643, 1024, 1012, 1024 \n",
            "n, m1, m2, m3 = 644, 1024, 1012, 1024 \n",
            "n, m1, m2, m3 = 645, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 646, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 647, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 648, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 649, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 650, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 651, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 652, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 653, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 654, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 655, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 656, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 657, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 658, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 659, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 660, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 661, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 662, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 663, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 664, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 665, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 666, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 667, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 668, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 669, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 670, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 671, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 672, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 673, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 674, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 675, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 676, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 677, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 678, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 679, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 680, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 681, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 682, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 683, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 684, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 685, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 686, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 687, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 688, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 689, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 690, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 691, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 692, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 693, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 694, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 695, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 696, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 697, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 698, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 699, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 700, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 701, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 702, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 703, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 704, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 705, 1024, 1017, 1024 \n",
            "n, m1, m2, m3 = 706, 1024, 1018, 1024 \n",
            "n, m1, m2, m3 = 707, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 708, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 709, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 710, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 711, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 712, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 713, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 714, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 715, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 716, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 717, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 718, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 719, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 720, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 721, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 722, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 723, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 724, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 725, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 726, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 727, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 728, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 729, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 730, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 731, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 732, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 733, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 734, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 735, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 736, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 737, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 738, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 739, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 740, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 741, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 742, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 743, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 744, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 745, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 746, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 747, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 748, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 749, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 750, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 751, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 752, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 753, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 754, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 755, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 756, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 757, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 758, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 759, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 760, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 761, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 762, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 763, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 764, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 765, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 766, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 767, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 768, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 769, 1024, 993, 1024 \n",
            "n, m1, m2, m3 = 770, 1024, 994, 1024 \n",
            "n, m1, m2, m3 = 771, 1024, 996, 1024 \n",
            "n, m1, m2, m3 = 772, 1024, 996, 1024 \n",
            "n, m1, m2, m3 = 773, 1024, 1000, 1024 \n",
            "n, m1, m2, m3 = 774, 1024, 1000, 1024 \n",
            "n, m1, m2, m3 = 775, 1024, 1000, 1024 \n",
            "n, m1, m2, m3 = 776, 1024, 1000, 1024 \n",
            "n, m1, m2, m3 = 777, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 778, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 779, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 780, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 781, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 782, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 783, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 784, 1024, 1008, 1024 \n",
            "n, m1, m2, m3 = 785, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 786, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 787, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 788, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 789, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 790, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 791, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 792, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 793, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 794, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 795, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 796, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 797, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 798, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 799, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 800, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 801, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 802, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 803, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 804, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 805, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 806, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 807, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 808, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 809, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 810, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 811, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 812, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 813, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 814, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 815, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 816, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 817, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 818, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 819, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 820, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 821, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 822, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 823, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 824, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 825, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 826, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 827, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 828, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 829, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 830, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 831, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 832, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 833, 1024, 1017, 1024 \n",
            "n, m1, m2, m3 = 834, 1024, 1018, 1024 \n",
            "n, m1, m2, m3 = 835, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 836, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 837, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 838, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 839, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 840, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 841, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 842, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 843, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 844, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 845, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 846, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 847, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 848, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 849, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 850, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 851, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 852, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 853, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 854, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 855, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 856, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 857, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 858, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 859, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 860, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 861, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 862, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 863, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 864, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 865, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 866, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 867, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 868, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 869, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 870, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 871, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 872, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 873, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 874, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 875, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 876, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 877, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 878, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 879, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 880, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 881, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 882, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 883, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 884, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 885, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 886, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 887, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 888, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 889, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 890, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 891, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 892, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 893, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 894, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 895, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 896, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 897, 1024, 1009, 1024 \n",
            "n, m1, m2, m3 = 898, 1024, 1010, 1024 \n",
            "n, m1, m2, m3 = 899, 1024, 1012, 1024 \n",
            "n, m1, m2, m3 = 900, 1024, 1012, 1024 \n",
            "n, m1, m2, m3 = 901, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 902, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 903, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 904, 1024, 1016, 1024 \n",
            "n, m1, m2, m3 = 905, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 906, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 907, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 908, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 909, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 910, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 911, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 912, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 913, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 914, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 915, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 916, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 917, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 918, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 919, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 920, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 921, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 922, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 923, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 924, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 925, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 926, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 927, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 928, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 929, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 930, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 931, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 932, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 933, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 934, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 935, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 936, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 937, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 938, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 939, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 940, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 941, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 942, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 943, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 944, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 945, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 946, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 947, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 948, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 949, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 950, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 951, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 952, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 953, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 954, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 955, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 956, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 957, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 958, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 959, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 960, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 961, 1024, 1017, 1024 \n",
            "n, m1, m2, m3 = 962, 1024, 1018, 1024 \n",
            "n, m1, m2, m3 = 963, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 964, 1024, 1020, 1024 \n",
            "n, m1, m2, m3 = 965, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 966, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 967, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 968, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 969, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 970, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 971, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 972, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 973, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 974, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 975, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 976, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 977, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 978, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 979, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 980, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 981, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 982, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 983, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 984, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 985, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 986, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 987, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 988, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 989, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 990, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 991, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 992, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 993, 1024, 1021, 1024 \n",
            "n, m1, m2, m3 = 994, 1024, 1022, 1024 \n",
            "n, m1, m2, m3 = 995, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 996, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 997, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 998, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 999, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1000, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1001, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1002, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1003, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1004, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1005, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1006, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1007, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1008, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1009, 1024, 1023, 1024 \n",
            "n, m1, m2, m3 = 1010, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1011, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1012, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1013, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1014, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1015, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1016, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1017, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1018, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1019, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1020, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1021, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1022, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1023, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1024, 1024, 1024, 1024 \n",
            "n, m1, m2, m3 = 1025, 2048, 1921, 2048 \n",
            "n, m1, m2, m3 = 1026, 2048, 1922, 2048 \n",
            "n, m1, m2, m3 = 1027, 2048, 1924, 2048 \n",
            "n, m1, m2, m3 = 1028, 2048, 1924, 2048 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Plbx2ULMLqzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extending to handle non-power of 2"
      ],
      "metadata": {
        "id": "Q9My4q9FRE-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    int m3 = 1 << (32 - __builtin_clz(blockDim.x-1));\n",
        "    m3 = m3/2;\n",
        "    int diff = blockDim.x - m3;\n",
        "\n",
        "    if (tid < diff){\n",
        "      temp[tid] = g_idata[tid];\n",
        "      temp[tid] += g_idata[tid + m3];\n",
        "    }\n",
        "    else {\n",
        "      temp[tid] = g_idata[tid];\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=m3/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d)  temp[tid] += temp[tid+d];\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "    if (tid==0) g_odata[0] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 192;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float),\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",h_data[0]-sum);\n",
        "  printf(\"reduction result = %f\\n\",h_data[0]);\n",
        "  printf(\"reference result = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161c3528-4def-43a1-ffcb-68d7955a1f6a",
        "id": "tPRSQJOdRI-9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd668364-a133-4f69-8c30-f53bb6d342cb",
        "id": "yFJQoDR6RI-_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd099ee-d80f-408c-eb7c-ec8b2d3993fa",
        "id": "33xegjMURI_A"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "reduction error = 0.000000\n",
            "reduction result = 896.000000\n",
            "reference result = 896.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Reduction for Non-Power-of-Two Threads**\n",
        "\n",
        "1. **Find the Largest Power of 2 Less Than `blockDim.x`**\n",
        "   $$\n",
        "   m3 = 1 << (32 - \\text{__builtin_clz}(\\text{blockDim.x} - 1))\n",
        "   $$\n",
        "   $$\n",
        "   m3 = m3 / 2\n",
        "   $$\n",
        "   - Computes the largest power of 2 $\\leq$ `blockDim.x`.\n",
        "   - Uses **leading zero count (`__builtin_clz()`)** for fast computation.\n",
        "\n",
        "2. **Merge Extra Elements Beyond This Power of 2**\n",
        "   $$\n",
        "   \\text{temp}[tid] = g\\_idata[tid]\n",
        "   $$\n",
        "   $$\n",
        "   \\text{temp}[tid] += \\text{temp}[tid + d]\n",
        "   $$\n",
        "   - Threads beyond `m3` are **added** to the corresponding lower-indexed threads.\n",
        "   - Reduction is performed in a **binary tree fashion**.\n",
        "\n",
        "4. **Store the Final Result in Global Memory**\n",
        "   $$\n",
        "   g\\_odata[0] = \\text{temp}[0]\n",
        "   $$\n",
        "   - **Only thread 0 writes the final sum**.\n",
        "\n",
        "\n",
        "## **Testing**\n",
        "- Set `num_threads = 192` (not a power of two).\n",
        "- Compared GPU result with CPU result (`reduction_gold()`).\n",
        "- Verified correctness by checking:\n",
        "  $$\n",
        "  \\text{reduction error} = h\\_data[0] - \\text{sum}\n",
        "  $$\n",
        "\n",
        "Since the **error is zero (~0.0f), the implementation is correct**.\n"
      ],
      "metadata": {
        "id": "hEmj88WT0whb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "5vuQFNWcZeVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tZt4cDTwZgh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### each block puts its partial sum into a different element of the output array, and then these are transferred to the host and summed there;"
      ],
      "metadata": {
        "id": "4Bmgvx1YzaEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int global_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[global_id];\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d) {\n",
        "     temp[tid] += temp[tid+d];  // Replace this line with:\n",
        "    }\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "    if (tid==0) g_odata[blockIdx.x] = temp[0];\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1000;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)*num_blocks) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "  cudaEventRecord(start);\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"kernel execution time: %.1f (ms) \\n\\n\", milli);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "  cudaDeviceSynchronize();\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float)*num_blocks,\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "  float gpu_sum = 0.0f;\n",
        "  for (int i=0; i<num_blocks; i++){\n",
        "    gpu_sum += h_data[i];\n",
        "  }\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",gpu_sum-sum);\n",
        "  printf(\"reduction result = %f\\n\",gpu_sum);\n",
        "  printf(\"reference result = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c0abdd-fc09-4cb7-c477-bdc5de69f9fb",
        "id": "6ZD00bhfZiwx"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94d3f0bd-108c-455d-9069-d83a76370366",
        "id": "PEFCgsXFZiwz"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "986b4724-4bee-4d65-9b33-3407763a4f39",
        "id": "D6zkn2HsZiwz"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "kernel execution time: 0.2 (ms) \n",
            "\n",
            "reduction error = 0.000000\n",
            "reduction result = 2305301.000000\n",
            "reference result = 2305301.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**First Solution: Partial Sum per Block**\n",
        "- Each block **reduces a section of the input** and **writes its sum to `g_odata[blockIdx.x]`**.\n",
        "- The **host sums all partial sums**.\n",
        "\n",
        "$$\n",
        "\\text{if} \\ (tid == 0), \\quad g\\_odata[blockIdx.x] = \\text{temp}[0]\n",
        "$$\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LaTcTeLADMAa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yUgmzB8dZgrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### an atomic addition is used to safely increment a single global sum."
      ],
      "metadata": {
        "id": "_Ux8fL9I0NHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "    extern  __shared__  float temp[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int global_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // first, each thread loads data into shared memory\n",
        "\n",
        "    temp[tid] = g_idata[global_id];\n",
        "    __syncthreads();\n",
        "\n",
        "    // next, we perform binary tree reduction\n",
        "\n",
        "    for (int d=blockDim.x/2; d>0; d=d/2) {\n",
        "      __syncthreads();  // ensure previous step completed\n",
        "      if (tid<d) {\n",
        "     temp[tid] += temp[tid+d];\n",
        "    }\n",
        "    }\n",
        "\n",
        "    // finally, first thread puts result into global memory\n",
        "    if (tid==0) atomicAdd(g_odata,temp[0]) ;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1000;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"kernel execution time: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "  cudaDeviceSynchronize();\n",
        "  float gpu_sum = 0.0f;\n",
        "  checkCudaErrors(cudaMemcpy(&gpu_sum, d_odata, sizeof(float),\n",
        "                            cudaMemcpyDeviceToHost));\n",
        "\n",
        "\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",gpu_sum-sum);\n",
        "  printf(\"reduction result gpu = %f\\n\",gpu_sum);\n",
        "  printf(\"reference result cpu = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAllSEa0Oa-",
        "outputId": "db8e4643-26be-4125-9730-e89af9e10ab1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628f1ab0-d277-40a7-9d60-18949081973c",
        "id": "7GKDebzk0S3_"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215e132a-5409-49c6-9a44-cf3551073e94",
        "id": "8wt04-Fi0S4B"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "kernel execution time: 0.1 (ms) \n",
            "\n",
            "reduction error = 0.000000\n",
            "reduction result gpu = 2305301.000000\n",
            "reference result cpu = 2305301.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Second Solution: Atomic Addition**\n",
        "- Each block **computes its partial sum**.\n",
        "- Instead of storing per-block sums, **an atomic addition updates a global sum**.\n",
        "$$\n",
        "\\text{if} \\ (tid == 0), \\quad \\text{atomicAdd}(g\\_odata, \\text{temp}[0])\n",
        "$$\n",
        "-no need for host code to do the summation\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nyQXt8e4DkSz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Q5A_6uU0Oqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "uVGdEUVKztQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modify the block-level reduction to use shuffle instructions as described in Lecture 4. Again your report should include your code, and results to show that the calculation has been carried out successfully"
      ],
      "metadata": {
        "id": "efGFX3LlzwAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    // dynamically allocated shared memory\n",
        "\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int global_id = blockIdx.x * blockDim.x + tid;\n",
        "    int warpIdx = tid / 32;\n",
        "    int thread_warp_idx = tid % 32;\n",
        "\n",
        "\n",
        "    float sum = g_idata[global_id];\n",
        "\n",
        "    for (int offset = warpSize/2; offset > 0; offset /= 2) {\n",
        "      sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);\n",
        "    }\n",
        "\n",
        "    // Thread 0 writes the final sum\n",
        "    if (thread_warp_idx == 0) {\n",
        "        g_odata[warpIdx]= sum;\n",
        "    }\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 1;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  int num_of_warps = ceil((float) num_threads/32) ;\n",
        "  printf(\"number of warps %i \\n\",num_of_warps);\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)*num_of_warps) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "  cudaDeviceSynchronize();\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float)*num_of_warps,\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "  float gpu_sum = 0.0f;\n",
        "  for (int i=0; i<num_of_warps; i++){\n",
        "    gpu_sum += h_data[i];\n",
        "  }\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",gpu_sum-sum);\n",
        "  printf(\"reduction result = %f\\n\",gpu_sum);\n",
        "  printf(\"reference result = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRbhdEUCzzTd",
        "outputId": "df4c4413-ffb2-43a5-a1ec-2620350a97a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b0bf274-4899-471d-c25a-c7fc7f355029",
        "id": "wxUPWVGH7yLz"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3f39d6-9dbb-41f0-eacb-6b3c601aee8e",
        "id": "FicE0ZGv7yL_"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "number of warps 16 \n",
            "reduction error = 0.000000\n",
            "reduction result = 2351.000000\n",
            "reference result = 2351.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YKrzKYEozziG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replacing Shared Memory with Warp Shuffle**\n",
        "- Each thread **loads its value** into a register:\n",
        "  $$\n",
        "  \\text{float sum} = g\\_idata[\\text{global_id}]\n",
        "  $$\n",
        "- Instead of using **shared memory and `__syncthreads()`**, warp shuffle **propagates values downwards**:\n",
        "  $$\n",
        "  \\text{sum} += \\_\\_shfl\\_down\\_sync(0xFFFFFFFF, \\text{sum}, \\text{offset})\n",
        "  $$\n",
        "  - This efficiently reduces values within a **warp (32 threads)**.\n",
        "\n",
        "---\n",
        "\n",
        "**Storing the Final Sum per Warp**\n",
        "- Only **thread 0 in each warp** stores the **warp sum**:\n",
        "  $$\n",
        "  \\text{if} (\\text{thread_warp_idx} == 0) \\quad g\\_odata[\\text{warpIdx}] = \\text{sum}\n",
        "  $$\n",
        "  - **`thread_warp_idx == 0` ensures that only one thread per warp writes.**\n",
        "  - This stores **one sum per warp** in `g_odata`.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Approach is More Efficient**\n",
        "- **Faster than shared memory** because data stays in registers.\n",
        "- **Avoids `__syncthreads()` overhead**, making execution more efficient.\n",
        "- **Uses warp-wide reduction**, minimizing memory transactions.\n",
        "\n",
        "**Limitations**\n",
        "- Works **only within a warp (32 threads)**. If we need block-wide reduction, an extra step is required.\n",
        "- Final summation across warps must be done separately (either in the host or with another kernel).\n",
        "\n",
        "---\n",
        "\n",
        "**Testing**\n",
        "- **Set `num_threads = 512`** → Each block has **512 threads = 16 warps**.\n",
        "- **Computed number of warps:**\n",
        "  $$\n",
        "  \\text{num_of_warps} = \\lceil \\frac{512}{32} \\rceil = 16\n",
        "  $$\n",
        "- **Final sum computed on the host**:\n",
        "  $$\n",
        "  \\text{gpu_sum} = \\sum_{i=0}^{\\text{num_of_warps}-1} h\\_data[i]\n",
        "  $$\n",
        "- **Compared with CPU reference (`reduction_gold()`):**\n",
        "  $$\n",
        "  \\text{reduction error} = h\\_data[0] - \\text{sum}\n",
        "  $$\n",
        "  - If the error is **zero (0.0f)**, the calculation is **correct**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TXfia0a1EiDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending to multiple blocks"
      ],
      "metadata": {
        "id": "Iz_tllz9Z8-4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJ9jr6BNaAQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reduction.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Practical 4 -- initial code for shared memory reduction for\n",
        "//                a single block which is a power of two in size\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "#include <float.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "float reduction_gold(float* idata, int len)\n",
        "{\n",
        "  float sum = 0.0f;\n",
        "  for(int i=0; i<len; i++) sum += idata[i];\n",
        "\n",
        "  return sum;\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void reduction(float *g_odata, float *g_idata)\n",
        "{\n",
        "    extern __shared__ float shared_mem[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int global_id = blockIdx.x * blockDim.x + tid;\n",
        "    int warpIdx = tid / 32;\n",
        "    int thread_warp_idx = tid % 32;\n",
        "\n",
        "    float sum = g_idata[global_id];\n",
        "\n",
        "    for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n",
        "        sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);\n",
        "    }\n",
        "\n",
        "    if (thread_warp_idx == 0) {\n",
        "        shared_mem[warpIdx] = sum;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    if (warpIdx == 0) {\n",
        "        sum = (tid < blockDim.x / warpSize) ? shared_mem[tid] : 0.0f;\n",
        "        for (int offset = warpSize / 2; offset > 0; offset /= 2) {\n",
        "            sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);\n",
        "        }\n",
        "\n",
        "        if (tid == 0) {\n",
        "            g_odata[blockIdx.x] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main( int argc, const char** argv)\n",
        "{\n",
        "  int num_blocks, num_threads, num_elements, mem_size, shared_mem_size;\n",
        "\n",
        "  float *h_data, *d_idata, *d_odata;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  num_blocks   = 128;  // start with only 1 thread block\n",
        "  num_threads  = 512;\n",
        "  num_elements = num_blocks*num_threads;\n",
        "  mem_size     = sizeof(float) * num_elements;\n",
        "\n",
        "  int num_of_warps = num_blocks * (num_threads/32) ;\n",
        "  printf(\"number of warps %i \\n\",num_of_warps);\n",
        "\n",
        "  // allocate host memory to store the input data\n",
        "  // and initialize to integer values between 0 and 10\n",
        "\n",
        "  h_data = (float*) malloc(mem_size);\n",
        "\n",
        "  for(int i = 0; i < num_elements; i++)\n",
        "    h_data[i] = floorf(10.0f*(rand()/(float)RAND_MAX));\n",
        "\n",
        "  // compute reference solution\n",
        "\n",
        "  float sum = reduction_gold(h_data, num_elements);\n",
        "\n",
        "  // allocate device memory input and output arrays\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_idata, mem_size) );\n",
        "  checkCudaErrors( cudaMalloc((void**)&d_odata, sizeof(float)*num_blocks) );\n",
        "\n",
        "  // copy host memory to device input array\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(d_idata, h_data, mem_size,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "\n",
        "  // execute the kernel\n",
        "\n",
        "  shared_mem_size = sizeof(float) * num_threads;\n",
        "\n",
        "  reduction<<<num_blocks,num_threads,shared_mem_size>>>(d_odata,d_idata);\n",
        "  getLastCudaError(\"reduction kernel execution failed\");\n",
        "\n",
        "  // copy result from device to host\n",
        "  cudaDeviceSynchronize();\n",
        "  checkCudaErrors( cudaMemcpy(h_data, d_odata, sizeof(float)*num_blocks,\n",
        "                              cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // check results\n",
        "  float gpu_sum = 0.0f;\n",
        "  for (int i=0; i<num_blocks; i++){\n",
        "    gpu_sum += h_data[i];\n",
        "  }\n",
        "\n",
        "  printf(\"reduction error = %f\\n\",gpu_sum-sum);\n",
        "  printf(\"reduction result = %f\\n\",gpu_sum);\n",
        "  printf(\"reference result = %f\\n\",sum);\n",
        "\n",
        "  // cleanup memory\n",
        "\n",
        "  free(h_data);\n",
        "  checkCudaErrors( cudaFree(d_idata) );\n",
        "  checkCudaErrors( cudaFree(d_odata) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6af509-8668-4bef-8af0-2269c7b7f3b5",
        "id": "ELtp55f6aD2-"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting reduction.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc reduction.cu -o reduction -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae6fa74-96d0-4b4c-9886-77441f716950",
        "id": "BF2jznXnaD2_"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z9reductionPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z9reductionPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 12 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b95b472-9a31-4225-d4bf-9396939e817a",
        "id": "aAoZctvuaD3A"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "number of warps 2048 \n",
            "reduction error = 0.000000\n",
            "reduction result = 294165.000000\n",
            "reference result = 294165.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Each **warp sum is stored in shared memory** before final block-wide reduction.\n",
        "\n",
        "- Instead of **storing per warp**, only **one final sum per block** is written:\n",
        "  $$\n",
        "  g\\_odata[\\text{blockIdx.x}] = \\text{final block sum}\n",
        "  $$\n",
        "  \n",
        "- The final summation step **properly accumulates all warp sums**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Algorithm**\n",
        "1. **Each thread loads data from global memory:**\n",
        "   $$\n",
        "   \\text{sum} = g\\_idata[\\text{global_id}]\n",
        "   $$\n",
        "\n",
        "2. **First reduction within each warp using `__shfl_down_sync()`:**\n",
        "   $$\n",
        "   \\text{sum} += \\_\\_shfl\\_down\\_sync(0xFFFFFFFF, \\text{sum}, \\text{offset})\n",
        "   $$\n",
        "\n",
        "3. **Thread 0 of each warp writes to shared memory:**\n",
        "   $$\n",
        "   \\text{if} \\ (\\text{thread\\_warp\\_idx} == 0) \\quad \\text{shared\\_mem}[\\text{warpIdx}] = \\text{sum}\n",
        "   $$\n",
        "\n",
        "4. **Final reduction across warps using warp shuffle:**\n",
        "   $$\n",
        "   \\text{sum} = \\text{shared\\_mem}[\\text{tid}]\n",
        "   $$\n",
        "   $$\n",
        "   \\text{sum} += \\_\\_shfl\\_down\\_sync(0xFFFFFFFF, \\text{sum}, \\text{offset})\n",
        "   $$\n",
        "\n",
        "5. **Only thread 0 writes final block sum to global memory:**\n",
        "   $$\n",
        "   \\text{if} \\ (\\text{tid} == 0) \\quad g\\_odata[\\text{blockIdx.x}] = \\text{sum}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "D-pj_JDfn9Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Un-Assign"
      ],
      "metadata": {
        "id": "l2TGtIPqRMrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the initial Practical 4 exercises. Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For the first exercise, it may be useful to know that the following line of code will round up the input n to the nearest power of 2, so then dividing it by 2 gives the largest power of 2 less than n.\n",
        "\n",
        "`for (m=1; m<n; m=2*m) {} `\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should again add your name to the title of the notebook (as in \"Practical 4 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n"
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "tLhEaicpji2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}