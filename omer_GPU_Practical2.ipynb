{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamoro98/Cuda-Programming/blob/main/omer_GPU_Practical2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Omer Kamal Ali Ebead</h1>"
      ],
      "metadata": {
        "id": "kjMuCdXL6EAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs, July 22-26, 2024**\n",
        "\n",
        "# **Practical 2**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify with the instruction below the details of the GPU which is available to you.  "
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "cb38014f-b371-4c9c-e82a-b91b4333fdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Feb  8 14:06:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "First we upload two header files from the course webpage."
      ],
      "metadata": {
        "id": "nlO6dHwW7gRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "8efb1f5c-5dd1-4ea0-a1be-94391bc221de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-08 14:06:34--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27832 (27K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "\rhelper_cuda.h         0%[                    ]       0  --.-KB/s               \rhelper_cuda.h       100%[===================>]  27.18K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-08 14:06:34 (2.09 MB/s) - ‘helper_cuda.h’ saved [27832/27832]\n",
            "\n",
            "--2025-02-08 14:06:35--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14875 (15K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  14.53K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-08 14:06:35 (1.13 MB/s) - ‘helper_string.h’ saved [14875/14875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1"
      ],
      "metadata": {
        "id": "vcYeps1U36xO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file prac2.cu."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile version1.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x; // why?\n",
        "\n",
        "  // version 2\n",
        "  //ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "9a645fd5-6baa-49de-ec72-59b7d786ad5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing version1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.  Note that the compilation links in the CUDA random number generation library cuRAND.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc version1.cu -o version1 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "ce16a725-3042-4935-a74e-9e47b4389743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 30 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./version1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "e267d668-d53a-459e-ff2f-dec5c8eb2c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 166.981628,  samples/sec: 1.149827e+10 \n",
            "Monte Carlo kernel execution time (ms): 29.542816 \n",
            "\n",
            "Average value and standard deviation of error  =    0.41786269    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5 - Version 2"
      ],
      "metadata": {
        "id": "DgR69vFzAmly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile version2.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  //ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x; // why?\n",
        "\n",
        "  // version 2\n",
        "  ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    //ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    //ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "9azlgRpC5gzT",
        "outputId": "d2e4aef2-ee3c-47c6-e4cb-3afed49f3b72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing version2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc version2.cu -o version2 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8523dd-4ac3-411b-da29-c4e57d26ec31",
        "id": "eGKx8iRm5wMp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 32 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./version2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d96b71d-1fc4-4fe8-abad-1ae578756f5e",
        "id": "41RY60V75wMr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 105.548225,  samples/sec: 1.819074e+10 \n",
            "Monte Carlo kernel execution time (ms): 100.579613 \n",
            "\n",
            "Average value and standard deviation of error  =    0.41793859    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version1 Kernal execution time : 29 ms\n",
        "\n",
        "Version2 Kernel execution time : 100 ms\n",
        "\n",
        "Version1 is faster than version 2"
      ],
      "metadata": {
        "id": "QIyYgqgk4C-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6\n",
        " why in Version 1 the 32 threads in a warp read in a consecutive block of 32 random numbers at the same time, but they don’t in Version 2.\n"
      ],
      "metadata": {
        "id": "2gnpbF_tBRBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Version 1, the index calculation for accessing elements in the random number array is:\n",
        "\n",
        "ind = threadIdx.x + 2 * N * blockIdx.x * blockDim.x;\n",
        "\n",
        "This means that each thread within a warp will access consecutive elements in the d_z array because threadIdx.x varies from 0 to 31 (within a warp), and since the addition happens before the multiplication by 2 * N, the threads will read a block of 32 consecutive values from memory at the same time. This results in coalesced memory access, where all 32 threads in a warp can efficiently fetch data in a single transaction, optimizing memory throughput.\n",
        "\n",
        "In Version 2, the index calculation is different:\n",
        "\n",
        "ind = 2 * N * threadIdx.x + 2 * N * blockIdx.x * blockDim.x;\n",
        "\n",
        "Here, the threadIdx.x is multiplied by 2 * N before being added to the block offset, meaning each thread will access memory locations that are strided by 2 * N. Since N is 100 , the stride is very wide. As a result, each thread in a warp accesses a widely separated memory location instead of fetching consecutive values. This leads to non-coalesced memory access, meaning that each memory transaction fetches data scattered across the memory space, resulting in poor performance due to unaligned and inefficient memory accesses.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xmF7isxaBeXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7\n",
        " Work out much data is being loaded from device memory by the kernel, and based on the execution time determine the effective transfer rate in GB/s. For the faster version of the kernel, is this close to the peak capability of the hardware (approximately 300GB/s for the T4 GPU) ?\n"
      ],
      "metadata": {
        "id": "cFOfuFg-DZyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_v1.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x; // why?\n",
        "\n",
        "  // version 2\n",
        "  //ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  printf(\"Total loaded data: %f bytes\\n\", (2.0f * h_N * NPATH ));\n",
        "  printf(\"Total data loaded in GB: %f GB\\n\", (2.0f * h_N * NPATH ) / pow(10.0f, 9));\n",
        "  printf(\"Effective transfer rate: %f GB/s\\n\", (2.0f * h_N * NPATH ) / (pow(10.0f, 9) * (milli / 1000.0f)));\n",
        "\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "EqFzaIUZEEs9",
        "outputId": "f04f9eb9-2ca3-41d8-ed08-8bea70579842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_v1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc data_v1.cu -o data_v1 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c8690e4-d3cd-45c6-a865-ae99ead0e555",
        "id": "BH0tjR2HE7sJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 30 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./data_v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1db538-6711-4fc2-dcdb-0cf33cb8f90f",
        "id": "qKNNmLcLE7sK"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 105.797539,  samples/sec: 1.814787e+10 \n",
            "Monte Carlo kernel execution time (ms): 29.519297 \n",
            "Total loaded data: 1920000000.000000 bytes\n",
            "Total data loaded in GB: 1.920000 GB\n",
            "Effective transfer rate: 65.042199 GB/s\n",
            "\n",
            "Average value and standard deviation of error  =    0.41786269    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_v2.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "  int   ind;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  //ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x; // why?\n",
        "\n",
        "  // version 2\n",
        "  ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  s1 = 1.0f;\n",
        "  s2 = 1.0f;\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    y1   = d_z[ind];\n",
        "    // version 1\n",
        "    //ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    y2   = rho*y1 + alpha*d_z[ind];\n",
        "    // version 1\n",
        "    //ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    ind += 1;\n",
        "\n",
        "    s1 = s1*(con1 + con2*y1);\n",
        "    s2 = s2*(con1 + con2*y2);\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "  payoff = 0.0f;\n",
        "  if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = payoff;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  double  sum1, sum2;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  printf(\"CURAND normal RNG  execution time (ms): %f,  samples/sec: %e \\n\",\n",
        "          milli, 2.0*h_N*NPATH/(0.001*milli));\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"Monte Carlo kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  printf(\"Total loaded data: %f bytes\\n\", (2.0f * h_N * NPATH ));\n",
        "  printf(\"Total data loaded in GB: %f GB\\n\", (2.0f * h_N * NPATH ) / pow(10.0f, 9));\n",
        "  printf(\"Effective transfer rate: %f GB/s\\n\", (2.0f * h_N * NPATH ) / (pow(10.0f, 9) * (milli / 1000.0f)));\n",
        "\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "41269TMxE7ZD",
        "outputId": "b125e5b9-0b1a-41bc-f42a-204f8616afaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data_v2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc data_v2.cu -o data_v2 -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13eef68-ffb9-4c1c-dabc-5ca21e0d6f20",
        "id": "HonLDBEU5Mwa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 36 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 32 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./data_v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2f93b2-a9a0-40ec-8eb4-4f9ad02b3e4a",
        "id": "wxab712D5Mwb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "CURAND normal RNG  execution time (ms): 105.867233,  samples/sec: 1.813592e+10 \n",
            "Monte Carlo kernel execution time (ms): 100.235069 \n",
            "Total loaded data: 1920000000.000000 bytes\n",
            "Total data loaded in GB: 1.920000 GB\n",
            "Effective transfer rate: 19.154973 GB/s\n",
            "\n",
            "Average value and standard deviation of error  =    0.41793859    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Version 1 (The fast one)\n",
        "\n",
        "Total loaded data: 1920000000.000000 bytes\n",
        "\n",
        "Total data loaded in GB: 1.920000 GB\n",
        "\n",
        "Effective transfer rate: 65.042199 GB/s\n",
        "\n",
        "Version 2\n",
        "\n",
        "Total loaded data: 1920000000.000000 bytes\n",
        "\n",
        "Total data loaded in GB: 1.920000 GB\n",
        "\n",
        "Effective transfer rate: 19.154973 GB/s\n"
      ],
      "metadata": {
        "id": "3eGKtxvU5Qvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "b65s7jU9D0fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile assign.cu\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ float   a,b,c,N;\n",
        "// __constant__ float  total;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routine\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "__global__ void pathcalc(float *d_z, float *d_v)\n",
        "{\n",
        "  int   ind;\n",
        "  float z;\n",
        "  float z_square;\n",
        "  float total = 0.0f;\n",
        "\n",
        "  // move array pointers to correct position\n",
        "\n",
        "  // version 1\n",
        "  ind = threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "  // version 2\n",
        "  // ind = 2*N*threadIdx.x + 2*N*blockIdx.x*blockDim.x;\n",
        "\n",
        "\n",
        "  // path calculation\n",
        "\n",
        "  for (int n=0; n<N; n++) {\n",
        "    z   = d_z[ind];\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "    z_square = z * z;\n",
        "\n",
        "    float y   = a*z_square + b*z + c ;\n",
        "    // version 1\n",
        "    ind += blockDim.x;      // shift pointer to next element\n",
        "    // version 2\n",
        "    //ind += 1;\n",
        "    total = total + y;\n",
        "\n",
        "  }\n",
        "\n",
        "  // put payoff value into device array\n",
        "\n",
        "\n",
        "  d_v[threadIdx.x + blockIdx.x*blockDim.x] = total/N;\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  float   h_a, h_b, h_c;\n",
        "  float  *h_v, *d_v, *d_z;\n",
        "  float  sum1;\n",
        "  int     NPATH=9600000;\n",
        "  float h_N = 100;\n",
        "\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_z, sizeof(float)*2*h_N*NPATH) );\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_a     = 1.0f;\n",
        "  h_b     = 2.0f;\n",
        "  h_c     = 3.0f;\n",
        "\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(a,    &h_a,    sizeof(h_a)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(b,    &h_b,    sizeof(h_b)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(c,    &h_c,    sizeof(h_c)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "\n",
        "\n",
        "  // random number generation\n",
        "\n",
        "  curandGenerator_t gen;\n",
        "  checkCudaErrors( curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT) );\n",
        "  checkCudaErrors( curandSetPseudoRandomGeneratorSeed(gen, 1234ULL) );\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( curandGenerateNormal(gen, d_z, 2*h_N*NPATH, 0.0f, 1.0f) );\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "\n",
        "\n",
        "  // execute kernel and time it\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<NPATH/128, 128>>>(d_z, d_v);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0f;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage   = %13.8f \\n\", sum1/NPATH);\n",
        "\n",
        "  // Tidy up library\n",
        "\n",
        "  checkCudaErrors( curandDestroyGenerator(gen) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "  checkCudaErrors( cudaFree(d_z) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "S6cpZlgT7OFq",
        "outputId": "a45db10f-637c-471c-e137-5d3b9cb83250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting assign.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc assign.cu -o assign -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "id": "pwxM5fYF7OJi",
        "outputId": "2da5e2aa-dd65-4148-8c53-987224d54fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem, 16 bytes cmem[3]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcPfS_' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcPfS_\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 16 registers, 368 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./assign"
      ],
      "metadata": {
        "id": "yWbh5bn35g_X",
        "outputId": "1e18ebbc-37fe-4cfe-fb39-4484f20e33f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "\n",
            "Average   =    4.00250053 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Codes"
      ],
      "metadata": {
        "id": "LjmxQwmrEAHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the Practical 2 exercises as prescribed in the Instructions for this practical.\n",
        "\n",
        "It is perhaps better to copy the Code cell (on my system this is done by using the mouse right-click) and paste it to form a new Code cell, so that the original cell uses Version 1, and the new cell uses Version 2.  You can then make a further copy when you do the new application for the final step in the practical.\n",
        "\n",
        "Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should add your name to the title of the notebook (as in \"Practical 2 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Lastly, the instructions below create, compile and execute a version of the code which generates the random numbers on-the-fly within each kernel using the device-level cuRAND library.  This gives the very best performance because the random numbers no longer are being transferred between the GPU and the device memory.\n"
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prac2_device.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// GPU version of Monte Carlo algorithm using NVIDIA's CURAND library\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// CUDA global constants\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__constant__ int   N;\n",
        "__constant__ float T, r, sigma, rho, alpha, dt, con1, con2;\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel routines -- see sections 3.5, 3.6 in cuRAND documentation\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void RNG_init(curandState *state)\n",
        "{\n",
        "  // RNG initialisation with id-based skipahead\n",
        "  int id = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  curand_init(1234, id, 0, &state[id]);\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void pathcalc(curandState *device_state, float *d_v,\n",
        "                         int mpath, int NPATH)\n",
        "{\n",
        "  float s1, s2, y1, y2, payoff;\n",
        "\n",
        "  int id = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "  curandState_t state = device_state[id];\n",
        "\n",
        "  for(int m=0; m<mpath; m++) {\n",
        "    s1 = 1.0f;\n",
        "    s2 = 1.0f;\n",
        "\n",
        "    for (int n=0; n<N; n++) {\n",
        "      y1 = curand_normal(&state);\n",
        "      y2 = rho*y1 + alpha*curand_normal(&state);\n",
        "\n",
        "      s1 = s1*(con1 + con2*y1);\n",
        "      s2 = s2*(con1 + con2*y2);\n",
        "    }\n",
        "\n",
        "    // put payoff value into device array\n",
        "\n",
        "    payoff = 0.0f;\n",
        "    if ( fabs(s1-1.0f)<0.1f && fabs(s2-1.0f)<0.1f ) payoff = exp(-r*T);\n",
        "\n",
        "    int payoff_id = id + m*gridDim.x*blockDim.x;\n",
        "    if (payoff_id < NPATH) d_v[payoff_id] = payoff;\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int     NPATH=9600000, h_N=100;\n",
        "  float   h_T, h_r, h_sigma, h_rho, h_alpha, h_dt, h_con1, h_con2;\n",
        "  float  *h_v, *d_v;\n",
        "  double  sum1, sum2;\n",
        "  curandState *state;\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory on host and device\n",
        "\n",
        "  h_v = (float *)malloc(sizeof(float)*NPATH);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_v, sizeof(float)*NPATH) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&state, sizeof(curandState)*NPATH) );\n",
        "\n",
        "  // printf(\"size of curandState is %d bytes\\n\",sizeof(curandState));\n",
        "\n",
        "  // define constants and transfer to GPU\n",
        "\n",
        "  h_T     = 1.0f;\n",
        "  h_r     = 0.05f;\n",
        "  h_sigma = 0.1f;\n",
        "  h_rho   = 0.5f;\n",
        "  h_alpha = sqrt(1.0f-h_rho*h_rho);\n",
        "  h_dt    = 1.0f/h_N;\n",
        "  h_con1  = 1.0f + h_r*h_dt;\n",
        "  h_con2  = sqrt(h_dt)*h_sigma;\n",
        "\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(N,    &h_N,    sizeof(h_N)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(T,    &h_T,    sizeof(h_T)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(r,    &h_r,    sizeof(h_r)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(sigma,&h_sigma,sizeof(h_sigma)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(rho,  &h_rho,  sizeof(h_rho)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(alpha,&h_alpha,sizeof(h_alpha)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(dt,   &h_dt,   sizeof(h_dt)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con1, &h_con1, sizeof(h_con1)) );\n",
        "  checkCudaErrors( cudaMemcpyToSymbol(con2, &h_con2, sizeof(h_con2)) );\n",
        "\n",
        "  // calculate theoretical occupancy -- see Pro Tip blog article:\n",
        "  // https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/\n",
        "\n",
        "  int device;\n",
        "  cudaDeviceProp props;\n",
        "  cudaGetDevice(&device);\n",
        "  cudaGetDeviceProperties(&props, device);\n",
        "\n",
        "  int maxActiveBlocks, blockSize=128;\n",
        "  cudaOccupancyMaxActiveBlocksPerMultiprocessor( &maxActiveBlocks,\n",
        "                                                 pathcalc, blockSize, 0);\n",
        "  printf(\"maxActiveBlocks/SM = %d \\n\",maxActiveBlocks);\n",
        "  printf(\"number of SMs      = %d \\n\",props.multiProcessorCount);\n",
        "  int blocks = maxActiveBlocks*props.multiProcessorCount;\n",
        "\n",
        "  // execute kernels\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  RNG_init<<<blocks, 128>>>(state);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"RNG_init execution failed\\n\");\n",
        "  printf(\"RNG_init kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  int paths_per_thread = (NPATH-1)/(128*blocks) + 1;\n",
        "  cudaEventRecord(start);\n",
        "  pathcalc<<<blocks, 128>>>(state,d_v,paths_per_thread,NPATH);\n",
        "  cudaEventRecord(stop);\n",
        "\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  getLastCudaError(\"pathcalc execution failed\\n\");\n",
        "  printf(\"pathcalc kernel execution time (ms): %f \\n\",milli);\n",
        "\n",
        "  // copy back results\n",
        "\n",
        "  checkCudaErrors( cudaMemcpy(h_v, d_v, sizeof(float)*NPATH,\n",
        "                   cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // compute average\n",
        "\n",
        "  sum1 = 0.0;\n",
        "  sum2 = 0.0;\n",
        "  for (int i=0; i<NPATH; i++) {\n",
        "    sum1 += h_v[i];\n",
        "    sum2 += h_v[i]*h_v[i];\n",
        "  }\n",
        "\n",
        "  printf(\"\\nAverage value and standard deviation of error  = %13.8f %13.8f\\n\\n\",\n",
        "\t sum1/NPATH, sqrt((sum2/NPATH - (sum1/NPATH)*(sum1/NPATH))/NPATH) );\n",
        "\n",
        "  // Release memory and exit cleanly\n",
        "\n",
        "  free(h_v);\n",
        "  checkCudaErrors( cudaFree(d_v) );\n",
        "\n",
        "  // CUDA exit -- needed to flush printf write buffer\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8xscLewbPlF",
        "outputId": "83ec0c6d-4e2b-4e72-ddaf-8aef590f3015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prac2_device.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc prac2_device.cu -o prac2_device -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart -lcurand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGXiohVT5OR",
        "outputId": "aacea8e3-57b2-4300-ade5-e0c4bf0d3182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 218048 bytes gmem, 108 bytes cmem[3], 64 bytes cmem[4]\n",
            "ptxas info    : Compiling entry function '_Z8pathcalcP17curandStateXORWOWPfii' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8pathcalcP17curandStateXORWOWPfii\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 29 registers, 376 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z8RNG_initP17curandStateXORWOW' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z8RNG_initP17curandStateXORWOW\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 32 registers, 360 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./prac2_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsfyCekjazPh",
        "outputId": "8a32f62b-9609-4c4b-d376-d03e74bfa70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "maxActiveBlocks/SM = 8 \n",
            "number of SMs      = 40 \n",
            "RNG_init kernel execution time (ms): 1.669120 \n",
            "pathcalc kernel execution time (ms): 23.672865 \n",
            "\n",
            "Average value and standard deviation of error  =    0.41802440    0.00015237\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again the final piece of code is to terminate the runtime."
      ],
      "metadata": {
        "id": "Fa5hiDuchY7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Q8twCtvWhd9q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}