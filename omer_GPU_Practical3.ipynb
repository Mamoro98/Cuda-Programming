{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mamoro98/Cuda-Programming/blob/main/omer_GPU_Practical3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Omer Kamal Ali Ebead</h1>"
      ],
      "metadata": {
        "id": "tKxW999Rh1g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA Programming on NVIDIA GPUs, July 22-26, 2024**\n",
        "\n",
        "# **Practical 3**\n",
        "\n",
        "Again make sure the correct Runtime is being used, by clicking on the Runtime option at the top, then \"Change runtime type\", and selecting an appropriate GPU such as the T4.\n",
        "\n",
        "Then verify with the instruction below the details of the GPU which is available to you.  "
      ],
      "metadata": {
        "id": "i1JlUA_e44zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uboEpcMD4xYA",
        "outputId": "9e798961-f608-482c-d8f8-c8952bae45dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 10 13:38:55 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "First we upload two header files from the course webpage."
      ],
      "metadata": {
        "id": "nlO6dHwW7gRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
        "!wget https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv1nyjTmTmr7",
        "outputId": "e794132c-200f-4184-9a73-c3ccddfd77e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 13:39:08--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_cuda.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27832 (27K) [text/x-chdr]\n",
            "Saving to: ‘helper_cuda.h’\n",
            "\n",
            "helper_cuda.h       100%[===================>]  27.18K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-02-10 13:39:09 (199 KB/s) - ‘helper_cuda.h’ saved [27832/27832]\n",
            "\n",
            "--2025-02-10 13:39:09--  https://people.maths.ox.ac.uk/gilesm/cuda/headers/helper_string.h\n",
            "Resolving people.maths.ox.ac.uk (people.maths.ox.ac.uk)... 129.67.184.129, 2001:630:441:202::8143:b881\n",
            "Connecting to people.maths.ox.ac.uk (people.maths.ox.ac.uk)|129.67.184.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14875 (15K) [text/x-chdr]\n",
            "Saving to: ‘helper_string.h’\n",
            "\n",
            "helper_string.h     100%[===================>]  14.53K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-02-10 13:39:10 (366 KB/s) - ‘helper_string.h’ saved [14875/14875]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The next step is to create the file laplace3d.cu which includes within it a reference C++ routine against which the CUDA results are compared."
      ],
      "metadata": {
        "id": "RD6IjBwY2Ltm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 16\n",
        "#define BLOCK_Y 16\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "                              const float* __restrict__ d_u1,\n",
        "                                    float* __restrict__ d_u2) // not allow to overlap\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  indg = i + j*NX;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  if ( i>=0 && i<=NX-1 && j>=0 && j<=NY-1 ) {\n",
        "\n",
        "    for (k=0; k<NZ; k++) {\n",
        "\n",
        "      if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "        u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "      }\n",
        "      else {\n",
        "        u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "             + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "             + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "      }\n",
        "      d_u2[indg] = u2;\n",
        "\n",
        "      indg += KOFF;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// same calc on cpu\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=512, NY=512, NZ=512,\n",
        "            REPEAT=20, bx, by, i, j, k;\n",
        "  float    *h_u1, *h_u2, *h_foo,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "\n",
        "  // problem heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeere !!\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "\n",
        "  dim3 dimGrid(bx,by);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  // if you want to check the error between the original array and the jacobia array\n",
        "  // checkCudaErrors( cudaMemcpy(h_u1, d_u2, bytes, cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  float err = 0.0;\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "        err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"rms error = %13f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcwQANS22i3Q",
        "outputId": "cb9ab33d-2e18-4870-8e5c-312f88e0552f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing laplace3d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We can now compile and run the executable.\n"
      ],
      "metadata": {
        "id": "yds03ug532rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d.cu -o laplace3d -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFHWm4Dd3_hw",
        "outputId": "cf807bf5-a78f-4357-b950-c1f594cc5890"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 64 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7jX9dSAaLj0",
        "outputId": "4204c56f-d654-4df6-d762-c2a8f5015e86"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 131.9 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 34737.8 (ms) \n",
            "\n",
            "20x GPU_laplace3d: 182.6 (ms) \n",
            "\n",
            "Copy u2 to host: 116.0 (ms) \n",
            "\n",
            "rms error =      0.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "W1s-ISmB7zuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDBqj2qU7y_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 16\n",
        "#define BLOCK_Y 16\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "                              const float* __restrict__ d_u1,\n",
        "                                    float* __restrict__ d_u2) // not allow to overlap\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  indg = i + j*NX;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  if ( i>=0 && i<=NX-1 && j>=0 && j<=NY-1 ) {\n",
        "\n",
        "    for (k=0; k<NZ; k++) {\n",
        "\n",
        "      if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "        u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "      }\n",
        "      else {\n",
        "        u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "             + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "             + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "      }\n",
        "      d_u2[indg] = u2;\n",
        "\n",
        "      indg += KOFF;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// same calc on cpu\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=1024, NY=1024, NZ=1024,\n",
        "            REPEAT=20, bx, by, i, j, k;\n",
        "  float    *h_u1, *h_u2,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  // for (i=0; i<REPEAT; i++) {\n",
        "    // Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    // h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  // }\n",
        "\n",
        "  // cudaEventRecord(stop);\n",
        "  // cudaEventSynchronize(stop);\n",
        "  // cudaEventElapsedTime(&milli, start, stop);\n",
        "  // printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "\n",
        "  // problem heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeere !!\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "\n",
        "  dim3 dimGrid(bx,by);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  // if you want to check the error between the original array and the jacobia array\n",
        "  // checkCudaErrors( cudaMemcpy(h_u1, d_u2, bytes, cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  // float err = 0.0;\n",
        "\n",
        "  //for (k=0; k<NZ; k++) {\n",
        "  //  for (j=0; j<NY; j++) {\n",
        "  //    for (i=0; i<NX; i++) {\n",
        "  //      ind = i + j*NX + k*NX*NY;\n",
        "  //      err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "  //    }\n",
        "  //  }\n",
        "  // }\n",
        "\n",
        "  // printf(\"rms error = %13f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89c613c-a083-47e6-d068-c546694ea9b6",
        "id": "2Hq2nACx74Rv"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace3d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d.cu -o laplace3d -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40338716-6918-4654-e9fd-51d7cb668c7b",
        "id": "zfYeFWqJ74Rx"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 64 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70c0bcc-8226-4873-a6aa-25cb7072f921",
        "id": "ynXLOXoN74Rx"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 1024 x 1024 x 1024 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 918.7 (ms) \n",
            "\n",
            "20x GPU_laplace3d: 1956.9 (ms) \n",
            "\n",
            "Copy u2 to host: 3636.2 (ms) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ROpjNg_7zCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "KRTS8Z2_8_3_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d2D-Z_X17zGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d.cu\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 128\n",
        "#define BLOCK_Y 8\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "                              const float* __restrict__ d_u1,\n",
        "                                    float* __restrict__ d_u2) // not allow to overlap\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  indg = i + j*NX;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  if ( i>=0 && i<=NX-1 && j>=0 && j<=NY-1 ) {\n",
        "\n",
        "    for (k=0; k<NZ; k++) {\n",
        "\n",
        "      if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "        u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "      }\n",
        "      else {\n",
        "        u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "             + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "             + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "      }\n",
        "      d_u2[indg] = u2;\n",
        "\n",
        "      indg += KOFF;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// same calc on cpu\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=1024, NY=1024, NZ=1024,\n",
        "            REPEAT=20, bx, by, i, j, k;\n",
        "  float    *h_u1, *h_u2,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  // for (i=0; i<REPEAT; i++) {\n",
        "    // Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    // h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  // }\n",
        "\n",
        "  // cudaEventRecord(stop);\n",
        "  // cudaEventSynchronize(stop);\n",
        "  // cudaEventElapsedTime(&milli, start, stop);\n",
        "  // printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "\n",
        "  // problem heeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeere !!\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "\n",
        "  dim3 dimGrid(bx,by);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  // if you want to check the error between the original array and the jacobia array\n",
        "  // checkCudaErrors( cudaMemcpy(h_u1, d_u2, bytes, cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  // float err = 0.0;\n",
        "\n",
        "  //for (k=0; k<NZ; k++) {\n",
        "  //  for (j=0; j<NY; j++) {\n",
        "  //    for (i=0; i<NX; i++) {\n",
        "  //      ind = i + j*NX + k*NX*NY;\n",
        "  //      err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "  //    }\n",
        "  //  }\n",
        "  // }\n",
        "\n",
        "  // printf(\"rms error = %13f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8767d983-6b7a-4cb8-946d-a2b91b192404",
        "id": "1jJCAqaZ9Ej1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace3d.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d.cu -o laplace3d -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e15c71d-7e08-4bd1-8359-3867a8a1ba16",
        "id": "tomdmiem9Ej2"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 64 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aeb2791-68fe-487d-9b5d-7229e1dd5a96",
        "id": "shWOddRD9Ej3"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 1024 x 1024 x 1024 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 916.9 (ms) \n",
            "\n",
            "20x GPU_laplace3d: 1229.6 (ms) \n",
            "\n",
            "Copy u2 to host: 3018.0 (ms) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline\n",
        "\\textbf{Block Size (X × Y)} & \\textbf{Execution Time (ms)} \\\\\n",
        "\\hline\n",
        "16 \\times 16  & 1956 \\\\\n",
        "32 \\times 32  & 1282 \\\\\n",
        "32 \\times 8   & 1259 \\\\\n",
        "64 \\times 8   & 1250 \\\\\n",
        "128 \\times 8  & 1229 \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "$$"
      ],
      "metadata": {
        "id": "DJpUQMAc_L8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "SlQ3AfXnGYYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "By going back to the previous code block you can modify the code to complete the initial Practical 3 exercises. Remember to first make your own copy of the notebook so that you are able to edit it.\n",
        "\n",
        "For students doing this as an assignment to be assessed, you should again add your name to the title of the notebook (as in \"Practical 3 -- Mike Giles.ipynb\"), make it shared (see the Share option in the top-right corner) and provide the shared link as the submission mechanism.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "For the later parts of Practical 3, the instructions below create, compile and execute a second version of the code in which each CUDA thread computes the value for just one 3D grid point."
      ],
      "metadata": {
        "id": "ncymVLmd4L82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d_new.cu\n",
        "\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 16\n",
        "#define BLOCK_Y 4\n",
        "#define BLOCK_Z 4\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "\t         \t      const float* __restrict__ d_u1,\n",
        "\t\t\t            float* __restrict__ d_u2)\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  k    = threadIdx.z + blockIdx.z*BLOCK_Z;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  indg = i + j*JOFF + k*KOFF;\n",
        "\n",
        "  if (i>=0 && i<=NX-1 && j>=0 && j<=NY-1 && k>=0 && k<=NZ-1) {\n",
        "    if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "      u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "    }\n",
        "    else {\n",
        "      u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "           + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "           + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "    }\n",
        "    d_u2[indg] = u2;\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=512, NY=512, NZ=512,\n",
        "            REPEAT=20, bx, by, bz, i, j, k;\n",
        "  float    *h_u1, *h_u2, *h_foo,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "  bz = 1 + (NZ-1)/BLOCK_Z;\n",
        "\n",
        "  dim3 dimGrid(bx,by,bz);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y,BLOCK_Z);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d_new: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  float err = 0.0;\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "        err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"rms error = %f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8xscLewbPlF",
        "outputId": "016bb1b0-383f-49dc-c3bc-b8d3b996719c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing laplace3d_new.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d_new.cu -o laplace3d_new -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGXiohVT5OR",
        "outputId": "5f455812-f5b2-4974-f522-c2d48e126e47"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 21 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsfyCekjazPh",
        "outputId": "9dfcc5c6-d85a-4f36-b1ca-b430098993b9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 113.9 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 27882.6 (ms) \n",
            "\n",
            "20x GPU_laplace3d_new: 155.1 (ms) \n",
            "\n",
            "Copy u2 to host: 118.3 (ms) \n",
            "\n",
            "rms error = 0.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimise its 3D thread block size, and compare its optimum running time to\n",
        "the original version"
      ],
      "metadata": {
        "id": "2OLCyKqpHsVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iyCA4p0fHrgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d_new.cu\n",
        "\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 32\n",
        "#define BLOCK_Y 2\n",
        "#define BLOCK_Z 4\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "\t         \t      const float* __restrict__ d_u1,\n",
        "\t\t\t            float* __restrict__ d_u2)\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  k    = threadIdx.z + blockIdx.z*BLOCK_Z;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  indg = i + j*JOFF + k*KOFF;\n",
        "\n",
        "  if (i>=0 && i<=NX-1 && j>=0 && j<=NY-1 && k>=0 && k<=NZ-1) {\n",
        "    if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "      u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "    }\n",
        "    else {\n",
        "      u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "           + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "           + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "    }\n",
        "    d_u2[indg] = u2;\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=512, NY=512, NZ=512,\n",
        "            REPEAT=20, bx, by, bz, i, j, k;\n",
        "  float    *h_u1, *h_u2, *h_foo,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "  bz = 1 + (NZ-1)/BLOCK_Z;\n",
        "\n",
        "  dim3 dimGrid(bx,by,bz);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y,BLOCK_Z);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d_new: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  float err = 0.0;\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "        err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"rms error = %f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9656c4e-339f-4910-d3e6-c7d95795cc6d",
        "id": "2lt5IfP-H14n"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace3d_new.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d_new.cu -o laplace3d_new -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97cb0cf4-44b5-4ace-dcac-3fe5397d75c2",
        "id": "EvLdrzXGH14p"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 21 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad410b04-09dc-4890-88a7-cbdea9d9270e",
        "id": "3KOzP1EsH14p"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 116.0 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 28642.8 (ms) \n",
            "\n",
            "20x GPU_laplace3d_new: 138.5 (ms) \n",
            "\n",
            "Copy u2 to host: 115.1 (ms) \n",
            "\n",
            "rms error = 0.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPrmC9XJHrjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{array}{|c|c|}\n",
        "\\hline\n",
        "\\textbf{Block Size (X × Y × Z)} & \\textbf{Execution Time (ms)} \\\\\n",
        "\\hline\n",
        "8 \\times 8 \\times 8   & 255 \\\\\n",
        "8 \\times 8 \\times 4   & 200 \\\\\n",
        "16 \\times 4 \\times 4  & 155 \\\\\n",
        "16 \\times 8 \\times 4  & 169 \\\\\n",
        "16 \\times 4 \\times 8  & 173 \\\\\n",
        "32 \\times 4 \\times 4  & 159 \\\\\n",
        "32 \\times 4 \\times 2  & 139 \\\\\n",
        "32 \\times 2 \\times 4  & 138 \\\\\n",
        "64 \\times 2 \\times 2  & 141 \\\\\n",
        "64 \\times 4 \\times 1  & 142 \\\\\n",
        "64 \\times 1 \\times 4  & 139 \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "IoHOpYOpIVVi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "5-UdDES5N8V6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "The next instructions check how many fp32 and integer instructions are performed by the two versions"
      ],
      "metadata": {
        "id": "5EZw1PFQOPLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --metrics \"smsp__sass_thread_inst_executed_op_fp32_pred_on.sum,smsp__sass_thread_inst_executed_op_integer_pred_on.sum\" ./laplace3d\n",
        "!ncu --metrics \"smsp__sass_thread_inst_executed_op_fp32_pred_on.sum,smsp__sass_thread_inst_executed_op_integer_pred_on.sum\" ./laplace3d_new"
      ],
      "metadata": {
        "id": "miPX3hreHrqm",
        "outputId": "d34a3015-a027-43a1-9405-bc5f6e996b5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 1024 x 1024 x 1024 \n",
            "\n",
            "==PROF== Connected to process 11080 (/content/laplace3d)\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 954.7 (ms) \n",
            "\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 0: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 1: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 2: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 3: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 4: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 5: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 6: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 7: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 8: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 9: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 10: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 11: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 12: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 13: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 14: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 15: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 16: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 17: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 18: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 19: 0%....50%....100% - 1 pass\n",
            "20x GPU_laplace3d: 36610.1 (ms) \n",
            "\n",
            "Copy u2 to host: 3670.8 (ms) \n",
            "\n",
            "==PROF== Disconnected from process 11080\n",
            "[11080] laplace3d@127.0.0.1\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    Metric Name                                            Metric Unit   Metric Value\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst  6,404,775,888\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 17,850,571,996\n",
            "    ------------------------------------------------------ ----------- --------------\n",
            "\n",
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "==PROF== Connected to process 11366 (/content/laplace3d_new)\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 126.3 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 40083.1 (ms) \n",
            "\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 0: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 1: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 2: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 3: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 4: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 5: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 6: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 7: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 8: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 9: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 10: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 11: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 12: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 13: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 14: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 15: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 16: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 17: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 18: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 19: 0%....50%....100% - 1 pass\n",
            "20x GPU_laplace3d_new: 10103.6 (ms) \n",
            "\n",
            "Copy u2 to host: 149.0 (ms) \n",
            "\n",
            "rms error = 0.000000 \n",
            "==PROF== Disconnected from process 11366\n",
            "[11366] laplace3d_new@127.0.0.1\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Command line profiler metrics\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    Metric Name                                            Metric Unit  Metric Value\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "    smsp__sass_thread_inst_executed_op_fp32_pred_on.sum           inst   795,906,000\n",
            "    smsp__sass_thread_inst_executed_op_integer_pred_on.sum        inst 8,296,431,488\n",
            "    ------------------------------------------------------ ----------- -------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tnDkimGrHrtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edMnRcsDHrwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmsPeIFHHrzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUuh0SdITB-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "J21jOk1aOYbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu ./laplace3d\n",
        "!ncu ./laplace3d_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa35a6e-cc24-4855-ef04-b24046f3310c",
        "id": "1USbXCV4TCpc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 1024 x 1024 x 1024 \n",
            "\n",
            "==PROF== Connected to process 11644 (/content/laplace3d)\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 1125.2 (ms) \n",
            "\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 0: 0%\n",
            "==WARNING== Backing up device memory in system memory. Kernel replay might be slow. Consider using \"--replay-mode application\" to avoid memory save-and-restore.\n",
            "....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 1: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 2: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 3: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 4: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 5: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 6: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 7: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 8: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 9: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 10: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 11: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 12: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 13: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 14: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 15: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 16: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 17: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 18: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 19: 0%....50%....100% - 9 passes\n",
            "20x GPU_laplace3d: 90384.1 (ms) \n",
            "\n",
            "Copy u2 to host: 3900.0 (ms) \n",
            "\n",
            "==PROF== Disconnected from process 11644\n",
            "[11644] laplace3d@127.0.0.1\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,313,829\n",
            "    Memory Throughput                 %         68.28\n",
            "    DRAM Throughput                   %         68.28\n",
            "    Duration                         ms        120.19\n",
            "    L1/TEX Cache Throughput           %         41.94\n",
            "    L2 Cache Throughput               %         13.73\n",
            "    SM Active Cycles              cycle 69,969,149.38\n",
            "    Compute (SM) Throughput           %         19.67\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.99\n",
            "    Achieved Active Warps Per SM           warp        27.84\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 410,035,649.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,804,068,352\n",
            "    Average L1 Active Cycles         cycle  69,969,149.38\n",
            "    Total L1 Elapsed Cycles          cycle  2,800,241,072\n",
            "    Average L2 Active Cycles         cycle 102,570,099.19\n",
            "    Total L2 Elapsed Cycles          cycle  3,288,523,680\n",
            "    Average SM Active Cycles         cycle  69,969,149.38\n",
            "    Total SM Elapsed Cycles          cycle  2,800,241,072\n",
            "    Average SMSP Active Cycles       cycle  68,935,521.99\n",
            "    Total SMSP Elapsed Cycles        cycle 11,200,964,288\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.02\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    69,939,243\n",
            "    Memory Throughput                 %         68.07\n",
            "    DRAM Throughput                   %         68.07\n",
            "    Duration                         ms        119.55\n",
            "    L1/TEX Cache Throughput           %         41.83\n",
            "    L2 Cache Throughput               %         13.81\n",
            "    SM Active Cycles              cycle 69,956,920.42\n",
            "    Compute (SM) Throughput           %         19.61\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.60\n",
            "    Achieved Active Warps Per SM           warp        27.71\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,255,914\n",
            "    Total DRAM Elapsed Cycles        cycle  4,797,767,680\n",
            "    Average L1 Active Cycles         cycle  69,956,920.42\n",
            "    Total L1 Elapsed Cycles          cycle  2,808,257,696\n",
            "    Average L2 Active Cycles         cycle 102,325,517.41\n",
            "    Total L2 Elapsed Cycles          cycle  3,271,004,608\n",
            "    Average SM Active Cycles         cycle  69,956,920.42\n",
            "    Total SM Elapsed Cycles          cycle  2,808,257,696\n",
            "    Average SMSP Active Cycles       cycle  68,770,347.96\n",
            "    Total SMSP Elapsed Cycles        cycle 11,233,030,784\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,038,572\n",
            "    Memory Throughput                 %         68.20\n",
            "    DRAM Throughput                   %         68.20\n",
            "    Duration                         ms        119.72\n",
            "    L1/TEX Cache Throughput           %         41.71\n",
            "    L2 Cache Throughput               %         13.78\n",
            "    SM Active Cycles              cycle 69,791,486.72\n",
            "    Compute (SM) Throughput           %         19.56\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.87\n",
            "    Achieved Active Warps Per SM           warp        27.80\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,293,872\n",
            "    Total DRAM Elapsed Cycles        cycle  4,789,134,336\n",
            "    Average L1 Active Cycles         cycle  69,791,486.72\n",
            "    Total L1 Elapsed Cycles          cycle  2,816,331,816\n",
            "    Average L2 Active Cycles         cycle 102,282,058.50\n",
            "    Total L2 Elapsed Cycles          cycle  3,275,650,080\n",
            "    Average SM Active Cycles         cycle  69,791,486.72\n",
            "    Total SM Elapsed Cycles          cycle  2,816,331,816\n",
            "    Average SMSP Active Cycles       cycle  69,076,577.22\n",
            "    Total SMSP Elapsed Cycles        cycle 11,265,327,264\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.01\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,068,569\n",
            "    Memory Throughput                 %         68.12\n",
            "    DRAM Throughput                   %         68.12\n",
            "    Duration                         ms        119.78\n",
            "    L1/TEX Cache Throughput           %         41.87\n",
            "    L2 Cache Throughput               %         13.81\n",
            "    SM Active Cycles              cycle 69,787,184.70\n",
            "    Compute (SM) Throughput           %         19.63\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.90\n",
            "    Achieved Active Warps Per SM           warp        27.81\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 408,528,260.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,797,568,000\n",
            "    Average L1 Active Cycles         cycle  69,787,184.70\n",
            "    Total L1 Elapsed Cycles          cycle  2,805,500,448\n",
            "    Average L2 Active Cycles         cycle 102,606,148.28\n",
            "    Total L2 Elapsed Cycles          cycle  3,277,053,088\n",
            "    Average SM Active Cycles         cycle  69,787,184.70\n",
            "    Total SM Elapsed Cycles          cycle  2,805,500,448\n",
            "    Average SMSP Active Cycles       cycle  68,797,297.76\n",
            "    Total SMSP Elapsed Cycles        cycle 11,222,001,792\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,042,843\n",
            "    Memory Throughput                 %         68.21\n",
            "    DRAM Throughput                   %         68.21\n",
            "    Duration                         ms        119.73\n",
            "    L1/TEX Cache Throughput           %         41.85\n",
            "    L2 Cache Throughput               %         13.78\n",
            "    SM Active Cycles              cycle 70,369,134.65\n",
            "    Compute (SM) Throughput           %         19.62\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.21\n",
            "    Achieved Active Warps Per SM           warp        27.59\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 408,582,878.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,792,072,192\n",
            "    Average L1 Active Cycles         cycle  70,369,134.65\n",
            "    Total L1 Elapsed Cycles          cycle  2,806,966,616\n",
            "    Average L2 Active Cycles         cycle 102,121,265.56\n",
            "    Total L2 Elapsed Cycles          cycle  3,275,849,856\n",
            "    Average SM Active Cycles         cycle  70,369,134.65\n",
            "    Total SM Elapsed Cycles          cycle  2,806,966,616\n",
            "    Average SMSP Active Cycles       cycle  68,970,231.81\n",
            "    Total SMSP Elapsed Cycles        cycle 11,227,866,464\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.98\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,306,614\n",
            "    Memory Throughput                 %         68.44\n",
            "    DRAM Throughput                   %         68.44\n",
            "    Duration                         ms        120.18\n",
            "    L1/TEX Cache Throughput           %         41.69\n",
            "    L2 Cache Throughput               %         13.71\n",
            "    SM Active Cycles              cycle 70,036,659.55\n",
            "    Compute (SM) Throughput           %         19.55\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.90\n",
            "    Achieved Active Warps Per SM           warp        27.81\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    409,845,950\n",
            "    Total DRAM Elapsed Cycles        cycle  4,790,604,800\n",
            "    Average L1 Active Cycles         cycle  70,036,659.55\n",
            "    Total L1 Elapsed Cycles          cycle  2,817,149,520\n",
            "    Average L2 Active Cycles         cycle 102,288,671.66\n",
            "    Total L2 Elapsed Cycles          cycle  3,288,185,984\n",
            "    Average SM Active Cycles         cycle  70,036,659.55\n",
            "    Total SM Elapsed Cycles          cycle  2,817,149,520\n",
            "    Average SMSP Active Cycles       cycle  69,254,077.59\n",
            "    Total SMSP Elapsed Cycles        cycle 11,268,598,080\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,217,910\n",
            "    Memory Throughput                 %         68.19\n",
            "    DRAM Throughput                   %         68.19\n",
            "    Duration                         ms        120.03\n",
            "    L1/TEX Cache Throughput           %         41.88\n",
            "    L2 Cache Throughput               %         13.74\n",
            "    SM Active Cycles              cycle 70,060,551.10\n",
            "    Compute (SM) Throughput           %         19.64\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.75\n",
            "    Achieved Active Warps Per SM           warp        27.76\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    409,414,510\n",
            "    Total DRAM Elapsed Cycles        cycle  4,803,497,984\n",
            "    Average L1 Active Cycles         cycle  70,060,551.10\n",
            "    Total L1 Elapsed Cycles          cycle  2,804,367,216\n",
            "    Average L2 Active Cycles         cycle 102,619,807.38\n",
            "    Total L2 Elapsed Cycles          cycle  3,284,036,896\n",
            "    Average SM Active Cycles         cycle  70,060,551.10\n",
            "    Total SM Elapsed Cycles          cycle  2,804,367,216\n",
            "    Average SMSP Active Cycles       cycle  68,857,795.13\n",
            "    Total SMSP Elapsed Cycles        cycle 11,217,468,864\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.03\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    69,915,952\n",
            "    Memory Throughput                 %         67.82\n",
            "    DRAM Throughput                   %         67.82\n",
            "    Duration                         ms        119.51\n",
            "    L1/TEX Cache Throughput           %         41.72\n",
            "    L2 Cache Throughput               %         13.86\n",
            "    SM Active Cycles              cycle 69,791,200.75\n",
            "    Compute (SM) Throughput           %         19.56\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.73\n",
            "    Achieved Active Warps Per SM           warp        27.75\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    407,712,402\n",
            "    Total DRAM Elapsed Cycles        cycle  4,809,504,768\n",
            "    Average L1 Active Cycles         cycle  69,791,200.75\n",
            "    Total L1 Elapsed Cycles          cycle  2,815,785,832\n",
            "    Average L2 Active Cycles         cycle 102,157,893.38\n",
            "    Total L2 Elapsed Cycles          cycle  3,269,914,976\n",
            "    Average SM Active Cycles         cycle  69,791,200.75\n",
            "    Total SM Elapsed Cycles          cycle  2,815,785,832\n",
            "    Average SMSP Active Cycles       cycle  68,839,575.06\n",
            "    Total SMSP Elapsed Cycles        cycle 11,263,143,328\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,246,880\n",
            "    Memory Throughput                 %         68.30\n",
            "    DRAM Throughput                   %         68.30\n",
            "    Duration                         ms        120.08\n",
            "    L1/TEX Cache Throughput           %         41.80\n",
            "    L2 Cache Throughput               %         13.75\n",
            "    SM Active Cycles              cycle 69,790,422.40\n",
            "    Compute (SM) Throughput           %         19.60\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.15\n",
            "    Achieved Active Warps Per SM           warp        27.89\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 409,899,031.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,801,259,520\n",
            "    Average L1 Active Cycles         cycle  69,790,422.40\n",
            "    Total L1 Elapsed Cycles          cycle  2,810,376,480\n",
            "    Average L2 Active Cycles         cycle 102,381,066.97\n",
            "    Total L2 Elapsed Cycles          cycle  3,285,392,480\n",
            "    Average SM Active Cycles         cycle  69,790,422.40\n",
            "    Total SM Elapsed Cycles          cycle  2,810,376,480\n",
            "    Average SMSP Active Cycles       cycle  68,682,064.16\n",
            "    Total SMSP Elapsed Cycles        cycle 11,241,505,920\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,068,360\n",
            "    Memory Throughput                 %         68.26\n",
            "    DRAM Throughput                   %         68.26\n",
            "    Duration                         ms        119.78\n",
            "    L1/TEX Cache Throughput           %         41.97\n",
            "    L2 Cache Throughput               %         13.76\n",
            "    SM Active Cycles              cycle 70,139,472.92\n",
            "    Compute (SM) Throughput           %         19.68\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.48\n",
            "    Achieved Active Warps Per SM           warp        27.68\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,727,932\n",
            "    Total DRAM Elapsed Cycles        cycle  4,789,950,464\n",
            "    Average L1 Active Cycles         cycle  70,139,472.92\n",
            "    Total L1 Elapsed Cycles          cycle  2,798,725,800\n",
            "    Average L2 Active Cycles         cycle 102,604,656.97\n",
            "    Total L2 Elapsed Cycles          cycle  3,277,042,560\n",
            "    Average SM Active Cycles         cycle  70,139,472.92\n",
            "    Total SM Elapsed Cycles          cycle  2,798,725,800\n",
            "    Average SMSP Active Cycles       cycle  69,156,986.22\n",
            "    Total SMSP Elapsed Cycles        cycle 11,194,903,200\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.02\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,065,117\n",
            "    Memory Throughput                 %         67.91\n",
            "    DRAM Throughput                   %         67.91\n",
            "    Duration                         ms        119.77\n",
            "    L1/TEX Cache Throughput           %         41.93\n",
            "    L2 Cache Throughput               %         13.80\n",
            "    SM Active Cycles              cycle 70,061,487.50\n",
            "    Compute (SM) Throughput           %         19.66\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.59\n",
            "    Achieved Active Warps Per SM           warp        27.71\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,619,051\n",
            "    Total DRAM Elapsed Cycles        cycle  4,813,795,328\n",
            "    Average L1 Active Cycles         cycle  70,061,487.50\n",
            "    Total L1 Elapsed Cycles          cycle  2,801,271,288\n",
            "    Average L2 Active Cycles         cycle 102,368,399.97\n",
            "    Total L2 Elapsed Cycles          cycle  3,276,891,616\n",
            "    Average SM Active Cycles         cycle  70,061,487.50\n",
            "    Total SM Elapsed Cycles          cycle  2,801,271,288\n",
            "    Average SMSP Active Cycles       cycle  68,882,885.61\n",
            "    Total SMSP Elapsed Cycles        cycle 11,205,085,152\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.98\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,043,433\n",
            "    Memory Throughput                 %         68.59\n",
            "    DRAM Throughput                   %         68.59\n",
            "    Duration                         ms        119.73\n",
            "    L1/TEX Cache Throughput           %         41.83\n",
            "    L2 Cache Throughput               %         13.74\n",
            "    SM Active Cycles              cycle 70,161,605.45\n",
            "    Compute (SM) Throughput           %         19.61\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.50\n",
            "    Achieved Active Warps Per SM           warp        27.68\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,589,909\n",
            "    Total DRAM Elapsed Cycles        cycle  4,765,662,208\n",
            "    Average L1 Active Cycles         cycle  70,161,605.45\n",
            "    Total L1 Elapsed Cycles          cycle  2,807,957,088\n",
            "    Average L2 Active Cycles         cycle 102,738,137.22\n",
            "    Total L2 Elapsed Cycles          cycle  3,275,876,768\n",
            "    Average SM Active Cycles         cycle  70,161,605.45\n",
            "    Total SM Elapsed Cycles          cycle  2,807,957,088\n",
            "    Average SMSP Active Cycles       cycle  68,754,971.03\n",
            "    Total SMSP Elapsed Cycles        cycle 11,231,828,352\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,341,495\n",
            "    Memory Throughput                 %         68.21\n",
            "    DRAM Throughput                   %         68.21\n",
            "    Duration                         ms        120.24\n",
            "    L1/TEX Cache Throughput           %         41.76\n",
            "    L2 Cache Throughput               %         13.74\n",
            "    SM Active Cycles              cycle 69,882,654.03\n",
            "    Compute (SM) Throughput           %         19.58\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.17\n",
            "    Achieved Active Warps Per SM           warp        27.90\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    410,149,434\n",
            "    Total DRAM Elapsed Cycles        cycle  4,810,561,536\n",
            "    Average L1 Active Cycles         cycle  69,882,654.03\n",
            "    Total L1 Elapsed Cycles          cycle  2,812,531,504\n",
            "    Average L2 Active Cycles         cycle 102,402,737.31\n",
            "    Total L2 Elapsed Cycles          cycle  3,289,817,632\n",
            "    Average SM Active Cycles         cycle  69,882,654.03\n",
            "    Total SM Elapsed Cycles          cycle  2,812,531,504\n",
            "    Average SMSP Active Cycles       cycle  68,852,192.21\n",
            "    Total SMSP Elapsed Cycles        cycle 11,250,126,016\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.99\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,150,452\n",
            "    Memory Throughput                 %         68.39\n",
            "    DRAM Throughput                   %         68.39\n",
            "    Duration                         ms        119.92\n",
            "    L1/TEX Cache Throughput           %         42.06\n",
            "    L2 Cache Throughput               %         13.75\n",
            "    SM Active Cycles              cycle 69,667,473.65\n",
            "    Compute (SM) Throughput           %         19.72\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.17\n",
            "    Achieved Active Warps Per SM           warp        27.89\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    408,926,059\n",
            "    Total DRAM Elapsed Cycles        cycle  4,783,275,008\n",
            "    Average L1 Active Cycles         cycle  69,667,473.65\n",
            "    Total L1 Elapsed Cycles          cycle  2,792,722,336\n",
            "    Average L2 Active Cycles         cycle 102,179,823.41\n",
            "    Total L2 Elapsed Cycles          cycle  3,280,881,952\n",
            "    Average SM Active Cycles         cycle  69,667,473.65\n",
            "    Total SM Elapsed Cycles          cycle  2,792,722,336\n",
            "    Average SMSP Active Cycles       cycle  68,759,368.44\n",
            "    Total SMSP Elapsed Cycles        cycle 11,170,889,344\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.99\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,238,101\n",
            "    Memory Throughput                 %         68.35\n",
            "    DRAM Throughput                   %         68.35\n",
            "    Duration                         ms        120.07\n",
            "    L1/TEX Cache Throughput           %         41.74\n",
            "    L2 Cache Throughput               %         13.75\n",
            "    SM Active Cycles              cycle 69,923,764.72\n",
            "    Compute (SM) Throughput           %         19.57\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.98\n",
            "    Achieved Active Warps Per SM           warp        27.83\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 409,495,401.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,792,915,968\n",
            "    Average L1 Active Cycles         cycle  69,923,764.72\n",
            "    Total L1 Elapsed Cycles          cycle  2,814,544,336\n",
            "    Average L2 Active Cycles         cycle 102,460,304.31\n",
            "    Total L2 Elapsed Cycles          cycle  3,284,981,920\n",
            "    Average SM Active Cycles         cycle  69,923,764.72\n",
            "    Total SM Elapsed Cycles          cycle  2,814,544,336\n",
            "    Average SMSP Active Cycles       cycle  69,101,609.49\n",
            "    Total SMSP Elapsed Cycles        cycle 11,258,177,344\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.98\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,254,511\n",
            "    Memory Throughput                 %         68.48\n",
            "    DRAM Throughput                   %         68.48\n",
            "    Duration                         ms        120.09\n",
            "    L1/TEX Cache Throughput           %         41.88\n",
            "    L2 Cache Throughput               %         13.73\n",
            "    SM Active Cycles              cycle 69,984,920.20\n",
            "    Compute (SM) Throughput           %         19.64\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.89\n",
            "    Achieved Active Warps Per SM           warp        27.80\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 409,444,048.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,783,136,768\n",
            "    Average L1 Active Cycles         cycle  69,984,920.20\n",
            "    Total L1 Elapsed Cycles          cycle  2,804,758,224\n",
            "    Average L2 Active Cycles         cycle 102,647,986.09\n",
            "    Total L2 Elapsed Cycles          cycle  3,285,749,280\n",
            "    Average SM Active Cycles         cycle  69,984,920.20\n",
            "    Total SM Elapsed Cycles          cycle  2,804,758,224\n",
            "    Average SMSP Active Cycles       cycle  69,095,641.74\n",
            "    Total SMSP Elapsed Cycles        cycle 11,219,032,896\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.99\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,213,317\n",
            "    Memory Throughput                 %         68.35\n",
            "    DRAM Throughput                   %         68.35\n",
            "    Duration                         ms        120.02\n",
            "    L1/TEX Cache Throughput           %         41.80\n",
            "    L2 Cache Throughput               %         13.74\n",
            "    SM Active Cycles              cycle 69,883,782.67\n",
            "    Compute (SM) Throughput           %         19.60\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.01\n",
            "    Achieved Active Warps Per SM           warp        27.84\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle    409,743,328\n",
            "    Total DRAM Elapsed Cycles        cycle  4,795,978,752\n",
            "    Average L1 Active Cycles         cycle  69,883,782.67\n",
            "    Total L1 Elapsed Cycles          cycle  2,809,991,112\n",
            "    Average L2 Active Cycles         cycle 102,764,052.69\n",
            "    Total L2 Elapsed Cycles          cycle  3,283,822,816\n",
            "    Average SM Active Cycles         cycle  69,883,782.67\n",
            "    Total SM Elapsed Cycles          cycle  2,809,991,112\n",
            "    Average SMSP Active Cycles       cycle  68,890,064.66\n",
            "    Total SMSP Elapsed Cycles        cycle 11,239,964,448\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.99\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,155,050\n",
            "    Memory Throughput                 %         68.34\n",
            "    DRAM Throughput                   %         68.34\n",
            "    Duration                         ms        119.92\n",
            "    L1/TEX Cache Throughput           %         41.94\n",
            "    L2 Cache Throughput               %         13.75\n",
            "    SM Active Cycles              cycle 69,827,675.97\n",
            "    Compute (SM) Throughput           %         19.66\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.01\n",
            "    Achieved Active Warps Per SM           warp        27.84\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 409,001,066.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,787,853,312\n",
            "    Average L1 Active Cycles         cycle  69,827,675.97\n",
            "    Total L1 Elapsed Cycles          cycle  2,800,967,464\n",
            "    Average L2 Active Cycles         cycle 102,682,400.38\n",
            "    Total L2 Elapsed Cycles          cycle  3,281,097,600\n",
            "    Average SM Active Cycles         cycle  69,827,675.97\n",
            "    Total SM Elapsed Cycles          cycle  2,800,967,464\n",
            "    Average SMSP Active Cycles       cycle  68,989,084.89\n",
            "    Total SMSP Elapsed Cycles        cycle 11,203,869,856\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          5.00\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,102,357\n",
            "    Memory Throughput                 %         68.34\n",
            "    DRAM Throughput                   %         68.34\n",
            "    Duration                         ms        119.83\n",
            "    L1/TEX Cache Throughput           %         41.82\n",
            "    L2 Cache Throughput               %         13.77\n",
            "    SM Active Cycles              cycle 69,994,880.60\n",
            "    Compute (SM) Throughput           %         19.61\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        86.74\n",
            "    Achieved Active Warps Per SM           warp        27.76\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 409,084,473.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,789,161,984\n",
            "    Average L1 Active Cycles         cycle  69,994,880.60\n",
            "    Total L1 Elapsed Cycles          cycle  2,808,526,808\n",
            "    Average L2 Active Cycles         cycle 102,690,253.12\n",
            "    Total L2 Elapsed Cycles          cycle  3,278,633,184\n",
            "    Average SM Active Cycles         cycle  69,994,880.60\n",
            "    Total SM Elapsed Cycles          cycle  2,808,526,808\n",
            "    Average SMSP Active Cycles       cycle  69,011,738.81\n",
            "    Total SMSP Elapsed Cycles        cycle 11,234,107,232\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (35, 35, 1)x(30, 30, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- -------------\n",
            "    Metric Name             Metric Unit  Metric Value\n",
            "    ----------------------- ----------- -------------\n",
            "    DRAM Frequency                  Ghz          4.96\n",
            "    SM Frequency                    Mhz        585.00\n",
            "    Elapsed Cycles                cycle    70,527,367\n",
            "    Memory Throughput                 %         68.68\n",
            "    DRAM Throughput                   %         68.68\n",
            "    Duration                         ms        120.56\n",
            "    L1/TEX Cache Throughput           %         41.85\n",
            "    L2 Cache Throughput               %         13.68\n",
            "    SM Active Cycles              cycle 69,927,972.70\n",
            "    Compute (SM) Throughput           %         19.62\n",
            "    ----------------------- ----------- -------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   900\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                  1,225\n",
            "    Registers Per Thread             register/thread              64\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread       1,102,500\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                               30.62\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 3.017%                                                                                          \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 900    \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            1\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            1\n",
            "    Theoretical Active Warps per SM        warp           29\n",
            "    Theoretical Occupancy                     %        90.62\n",
            "    Achieved Occupancy                        %        87.31\n",
            "    Achieved Active Warps Per SM           warp        27.94\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- --------------\n",
            "    Metric Name                Metric Unit   Metric Value\n",
            "    -------------------------- ----------- --------------\n",
            "    Average DRAM Active Cycles       cycle 410,979,957.50\n",
            "    Total DRAM Elapsed Cycles        cycle  4,786,891,776\n",
            "    Average L1 Active Cycles         cycle  69,927,972.70\n",
            "    Total L1 Elapsed Cycles          cycle  2,806,907,704\n",
            "    Average L2 Active Cycles         cycle 102,395,739.88\n",
            "    Total L2 Elapsed Cycles          cycle  3,298,510,048\n",
            "    Average SM Active Cycles         cycle  69,927,972.70\n",
            "    Total SM Elapsed Cycles          cycle  2,806,907,704\n",
            "    Average SMSP Active Cycles       cycle  68,961,232.06\n",
            "    Total SMSP Elapsed Cycles        cycle 11,227,630,816\n",
            "    -------------------------- ----------- --------------\n",
            "\n",
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "==PROF== Connected to process 12302 (/content/laplace3d_new)\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 118.6 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 39436.4 (ms) \n",
            "\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 0: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 1: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 2: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 3: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 4: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 5: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 6: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 7: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 8: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 9: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 10: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 11: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 12: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 13: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 14: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 15: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 16: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 17: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 18: 0%....50%....100% - 9 passes\n",
            "==PROF== Profiling \"GPU_laplace3d\" - 19: 0%....50%....100% - 9 passes\n",
            "20x GPU_laplace3d_new: 5306.1 (ms) \n",
            "\n",
            "Copy u2 to host: 116.3 (ms) \n",
            "\n",
            "rms error = 0.000000 \n",
            "==PROF== Disconnected from process 12302\n",
            "[12302] laplace3d_new@127.0.0.1\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,704\n",
            "    Memory Throughput                 %        72.61\n",
            "    DRAM Throughput                   %        72.61\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.75\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,613.85\n",
            "    Compute (SM) Throughput           %        58.16\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.87\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.13%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,890,675.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,244,608\n",
            "    Average L1 Active Cycles         cycle  4,012,613.85\n",
            "    Total L1 Elapsed Cycles          cycle   160,524,336\n",
            "    Average L2 Active Cycles         cycle  5,854,614.47\n",
            "    Total L2 Elapsed Cycles          cycle   187,717,216\n",
            "    Average SM Active Cycles         cycle  4,012,613.85\n",
            "    Total SM Elapsed Cycles          cycle   160,524,336\n",
            "    Average SMSP Active Cycles       cycle  4,012,741.08\n",
            "    Total SMSP Elapsed Cycles        cycle   642,097,344\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,267\n",
            "    Memory Throughput                 %        72.61\n",
            "    DRAM Throughput                   %        72.61\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.15\n",
            "    SM Active Cycles              cycle 4,012,349.05\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.87\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.13%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,892,577.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,259,968\n",
            "    Average L1 Active Cycles         cycle  4,012,349.05\n",
            "    Total L1 Elapsed Cycles          cycle   160,556,616\n",
            "    Average L2 Active Cycles         cycle  5,851,258.25\n",
            "    Total L2 Elapsed Cycles          cycle   187,696,256\n",
            "    Average SM Active Cycles         cycle  4,012,349.05\n",
            "    Total SM Elapsed Cycles          cycle   160,556,616\n",
            "    Average SMSP Active Cycles       cycle  4,012,938.42\n",
            "    Total SMSP Elapsed Cycles        cycle   642,226,464\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,015,667\n",
            "    Memory Throughput                 %        72.60\n",
            "    DRAM Throughput                   %        72.60\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.75\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle 4,012,532.70\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.93\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.07%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,893,810.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,300,928\n",
            "    Average L1 Active Cycles         cycle  4,012,532.70\n",
            "    Total L1 Elapsed Cycles          cycle   160,540,344\n",
            "    Average L2 Active Cycles         cycle     5,853,717\n",
            "    Total L2 Elapsed Cycles          cycle   187,808,960\n",
            "    Average SM Active Cycles         cycle  4,012,532.70\n",
            "    Total SM Elapsed Cycles          cycle   160,540,344\n",
            "    Average SMSP Active Cycles       cycle  4,011,494.90\n",
            "    Total SMSP Elapsed Cycles        cycle   642,161,376\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,015,051\n",
            "    Memory Throughput                 %        72.65\n",
            "    DRAM Throughput                   %        72.65\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.77\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle    4,011,872\n",
            "    Compute (SM) Throughput           %        58.18\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.93\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.07%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,893,644.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,140,160\n",
            "    Average L1 Active Cycles         cycle     4,011,872\n",
            "    Total L1 Elapsed Cycles          cycle   160,456,936\n",
            "    Average L2 Active Cycles         cycle  5,853,062.16\n",
            "    Total L2 Elapsed Cycles          cycle   187,779,872\n",
            "    Average SM Active Cycles         cycle     4,011,872\n",
            "    Total SM Elapsed Cycles          cycle   160,456,936\n",
            "    Average SMSP Active Cycles       cycle  4,012,381.39\n",
            "    Total SMSP Elapsed Cycles        cycle   641,827,744\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,015,269\n",
            "    Memory Throughput                 %        72.65\n",
            "    DRAM Throughput                   %        72.65\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.75\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle 4,013,709.98\n",
            "    Compute (SM) Throughput           %        58.16\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.89\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.11%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,889,968\n",
            "    Total DRAM Elapsed Cycles        cycle  274,098,176\n",
            "    Average L1 Active Cycles         cycle 4,013,709.98\n",
            "    Total L1 Elapsed Cycles          cycle  160,509,408\n",
            "    Average L2 Active Cycles         cycle 5,854,479.38\n",
            "    Total L2 Elapsed Cycles          cycle  187,789,632\n",
            "    Average SM Active Cycles         cycle 4,013,709.98\n",
            "    Total SM Elapsed Cycles          cycle  160,509,408\n",
            "    Average SMSP Active Cycles       cycle 4,012,785.41\n",
            "    Total SMSP Elapsed Cycles        cycle  642,037,632\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,545\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.74\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,010,676.27\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.94\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.06%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,891,683.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,201,600\n",
            "    Average L1 Active Cycles         cycle  4,010,676.27\n",
            "    Total L1 Elapsed Cycles          cycle   160,544,632\n",
            "    Average L2 Active Cycles         cycle  5,852,278.53\n",
            "    Total L2 Elapsed Cycles          cycle   187,755,968\n",
            "    Average SM Active Cycles         cycle  4,010,676.27\n",
            "    Total SM Elapsed Cycles          cycle   160,544,632\n",
            "    Average SMSP Active Cycles       cycle  4,011,597.15\n",
            "    Total SMSP Elapsed Cycles        cycle   642,178,528\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,644\n",
            "    Memory Throughput                 %        72.58\n",
            "    DRAM Throughput                   %        72.58\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,776.38\n",
            "    Compute (SM) Throughput           %        58.14\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.89\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.11%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,892,130.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,356,224\n",
            "    Average L1 Active Cycles         cycle  4,012,776.38\n",
            "    Total L1 Elapsed Cycles          cycle   160,567,792\n",
            "    Average L2 Active Cycles         cycle  5,855,024.22\n",
            "    Total L2 Elapsed Cycles          cycle   187,760,768\n",
            "    Average SM Active Cycles         cycle  4,012,776.38\n",
            "    Total SM Elapsed Cycles          cycle   160,567,792\n",
            "    Average SMSP Active Cycles       cycle  4,012,656.42\n",
            "    Total SMSP Elapsed Cycles        cycle   642,271,168\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,278\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.75\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,014,279.60\n",
            "    Compute (SM) Throughput           %        58.16\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.87\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.13%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,892,066.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,208,768\n",
            "    Average L1 Active Cycles         cycle  4,014,279.60\n",
            "    Total L1 Elapsed Cycles          cycle   160,522,112\n",
            "    Average L2 Active Cycles         cycle  5,851,077.16\n",
            "    Total L2 Elapsed Cycles          cycle   187,743,616\n",
            "    Average SM Active Cycles         cycle  4,014,279.60\n",
            "    Total SM Elapsed Cycles          cycle   160,522,112\n",
            "    Average SMSP Active Cycles       cycle  4,012,855.42\n",
            "    Total SMSP Elapsed Cycles        cycle   642,088,448\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,015,363\n",
            "    Memory Throughput                 %        72.65\n",
            "    DRAM Throughput                   %        72.65\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.76\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle 4,012,651.10\n",
            "    Compute (SM) Throughput           %        58.16\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.92\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.08%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,893,516\n",
            "    Total DRAM Elapsed Cycles        cycle  274,132,992\n",
            "    Average L1 Active Cycles         cycle 4,012,651.10\n",
            "    Total L1 Elapsed Cycles          cycle  160,502,120\n",
            "    Average L2 Active Cycles         cycle 5,854,486.12\n",
            "    Total L2 Elapsed Cycles          cycle  187,794,368\n",
            "    Average SM Active Cycles         cycle 4,012,651.10\n",
            "    Total SM Elapsed Cycles          cycle  160,502,120\n",
            "    Average SMSP Active Cycles       cycle 4,012,074.44\n",
            "    Total SMSP Elapsed Cycles        cycle  642,008,480\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,957\n",
            "    Memory Throughput                 %        72.63\n",
            "    DRAM Throughput                   %        72.63\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.74\n",
            "    L2 Cache Throughput               %        35.15\n",
            "    SM Active Cycles              cycle 4,013,213.02\n",
            "    Compute (SM) Throughput           %        58.14\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.87\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.13%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,891,352.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,161,664\n",
            "    Average L1 Active Cycles         cycle  4,013,213.02\n",
            "    Total L1 Elapsed Cycles          cycle   160,557,048\n",
            "    Average L2 Active Cycles         cycle  5,852,359.47\n",
            "    Total L2 Elapsed Cycles          cycle   187,728,672\n",
            "    Average SM Active Cycles         cycle  4,013,213.02\n",
            "    Total SM Elapsed Cycles          cycle   160,557,048\n",
            "    Average SMSP Active Cycles       cycle  4,011,650.16\n",
            "    Total SMSP Elapsed Cycles        cycle   642,228,192\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,845\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.75\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,918.17\n",
            "    Compute (SM) Throughput           %        58.16\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.88\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.12%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,893,625.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,240,512\n",
            "    Average L1 Active Cycles         cycle  4,012,918.17\n",
            "    Total L1 Elapsed Cycles          cycle   160,521,472\n",
            "    Average L2 Active Cycles         cycle  5,854,873.75\n",
            "    Total L2 Elapsed Cycles          cycle   187,723,456\n",
            "    Average SM Active Cycles         cycle  4,012,918.17\n",
            "    Total SM Elapsed Cycles          cycle   160,521,472\n",
            "    Average SMSP Active Cycles       cycle  4,012,628.91\n",
            "    Total SMSP Elapsed Cycles        cycle   642,085,888\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,015,171\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.15\n",
            "    SM Active Cycles              cycle 4,011,607.55\n",
            "    Compute (SM) Throughput           %        58.14\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.93\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.07%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,895,475.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,247,680\n",
            "    Average L1 Active Cycles         cycle  4,011,607.55\n",
            "    Total L1 Elapsed Cycles          cycle   160,557,216\n",
            "    Average L2 Active Cycles         cycle  5,851,327.59\n",
            "    Total L2 Elapsed Cycles          cycle   187,785,376\n",
            "    Average SM Active Cycles         cycle  4,011,607.55\n",
            "    Total SM Elapsed Cycles          cycle   160,557,216\n",
            "    Average SMSP Active Cycles       cycle  4,011,154.47\n",
            "    Total SMSP Elapsed Cycles        cycle   642,228,864\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,809\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.74\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,823.60\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.87\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.13%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,896,555\n",
            "    Total DRAM Elapsed Cycles        cycle  274,253,824\n",
            "    Average L1 Active Cycles         cycle 4,012,823.60\n",
            "    Total L1 Elapsed Cycles          cycle  160,534,992\n",
            "    Average L2 Active Cycles         cycle 5,853,584.12\n",
            "    Total L2 Elapsed Cycles          cycle  187,721,760\n",
            "    Average SM Active Cycles         cycle 4,012,823.60\n",
            "    Total SM Elapsed Cycles          cycle  160,534,992\n",
            "    Average SMSP Active Cycles       cycle 4,012,153.67\n",
            "    Total SMSP Elapsed Cycles        cycle  642,139,968\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,774\n",
            "    Memory Throughput                 %        72.60\n",
            "    DRAM Throughput                   %        72.60\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,401.88\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.92\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.08%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,892,050\n",
            "    Total DRAM Elapsed Cycles        cycle  274,298,880\n",
            "    Average L1 Active Cycles         cycle 4,012,401.88\n",
            "    Total L1 Elapsed Cycles          cycle  160,543,536\n",
            "    Average L2 Active Cycles         cycle 5,851,252.44\n",
            "    Total L2 Elapsed Cycles          cycle  187,766,912\n",
            "    Average SM Active Cycles         cycle 4,012,401.88\n",
            "    Total SM Elapsed Cycles          cycle  160,543,536\n",
            "    Average SMSP Active Cycles       cycle 4,011,731.81\n",
            "    Total SMSP Elapsed Cycles        cycle  642,174,144\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,254\n",
            "    Memory Throughput                 %        72.63\n",
            "    DRAM Throughput                   %        72.63\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,011,079.12\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.93\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.07%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,893,765.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,212,864\n",
            "    Average L1 Active Cycles         cycle  4,011,079.12\n",
            "    Total L1 Elapsed Cycles          cycle   160,546,536\n",
            "    Average L2 Active Cycles         cycle  5,856,246.50\n",
            "    Total L2 Elapsed Cycles          cycle   187,742,720\n",
            "    Average SM Active Cycles         cycle  4,011,079.12\n",
            "    Total SM Elapsed Cycles          cycle   160,546,536\n",
            "    Average SMSP Active Cycles       cycle  4,011,785.35\n",
            "    Total SMSP Elapsed Cycles        cycle   642,186,144\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,423\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.74\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,012,160.15\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.91\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.09%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,891,468\n",
            "    Total DRAM Elapsed Cycles        cycle  274,205,696\n",
            "    Average L1 Active Cycles         cycle 4,012,160.15\n",
            "    Total L1 Elapsed Cycles          cycle  160,556,944\n",
            "    Average L2 Active Cycles         cycle 5,853,859.12\n",
            "    Total L2 Elapsed Cycles          cycle  187,750,208\n",
            "    Average SM Active Cycles         cycle 4,012,160.15\n",
            "    Total SM Elapsed Cycles          cycle  160,556,944\n",
            "    Average SMSP Active Cycles       cycle 4,011,408.66\n",
            "    Total SMSP Elapsed Cycles        cycle  642,227,776\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,016,123\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.87\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle 4,012,481.30\n",
            "    Compute (SM) Throughput           %        58.14\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.95\n",
            "    Achieved Active Warps Per SM           warp        26.54\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.05%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,891,228.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,222,080\n",
            "    Average L1 Active Cycles         cycle  4,012,481.30\n",
            "    Total L1 Elapsed Cycles          cycle   160,561,664\n",
            "    Average L2 Active Cycles         cycle  5,853,115.91\n",
            "    Total L2 Elapsed Cycles          cycle   187,830,016\n",
            "    Average SM Active Cycles         cycle  4,012,481.30\n",
            "    Total SM Elapsed Cycles          cycle   160,561,664\n",
            "    Average SMSP Active Cycles       cycle  4,011,891.83\n",
            "    Total SMSP Elapsed Cycles        cycle   642,246,656\n",
            "    -------------------------- ----------- -------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,013,437\n",
            "    Memory Throughput                 %        72.63\n",
            "    DRAM Throughput                   %        72.63\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.15\n",
            "    SM Active Cycles              cycle 4,012,117.73\n",
            "    Compute (SM) Throughput           %        58.14\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.89\n",
            "    Achieved Active Warps Per SM           warp        26.52\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.11%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,893,547\n",
            "    Total DRAM Elapsed Cycles        cycle  274,204,672\n",
            "    Average L1 Active Cycles         cycle 4,012,117.73\n",
            "    Total L1 Elapsed Cycles          cycle  160,573,960\n",
            "    Average L2 Active Cycles         cycle 5,853,918.62\n",
            "    Total L2 Elapsed Cycles          cycle  187,704,288\n",
            "    Average SM Active Cycles         cycle 4,012,117.73\n",
            "    Total SM Elapsed Cycles          cycle  160,573,960\n",
            "    Average SMSP Active Cycles       cycle 4,012,674.94\n",
            "    Total SMSP Elapsed Cycles        cycle  642,295,840\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         4.99\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,776\n",
            "    Memory Throughput                 %        72.64\n",
            "    DRAM Throughput                   %        72.64\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.73\n",
            "    L2 Cache Throughput               %        35.13\n",
            "    SM Active Cycles              cycle 4,011,755.60\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.92\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.08%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle   24,894,475\n",
            "    Total DRAM Elapsed Cycles        cycle  274,151,424\n",
            "    Average L1 Active Cycles         cycle 4,011,755.60\n",
            "    Total L1 Elapsed Cycles          cycle  160,553,856\n",
            "    Average L2 Active Cycles         cycle 5,854,933.38\n",
            "    Total L2 Elapsed Cycles          cycle  187,766,816\n",
            "    Average SM Active Cycles         cycle 4,011,755.60\n",
            "    Total SM Elapsed Cycles          cycle  160,553,856\n",
            "    Average SMSP Active Cycles       cycle 4,012,068.23\n",
            "    Total SMSP Elapsed Cycles        cycle  642,215,424\n",
            "    -------------------------- ----------- ------------\n",
            "\n",
            "  GPU_laplace3d(long long, long long, long long, const float *, float *) (16, 256, 128)x(32, 2, 4), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       585.00\n",
            "    Elapsed Cycles                cycle    4,014,172\n",
            "    Memory Throughput                 %        72.62\n",
            "    DRAM Throughput                   %        72.62\n",
            "    Duration                         ms         6.86\n",
            "    L1/TEX Cache Throughput           %        61.74\n",
            "    L2 Cache Throughput               %        35.14\n",
            "    SM Active Cycles              cycle 4,011,991.27\n",
            "    Compute (SM) Throughput           %        58.15\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    \n",
            "          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       \n",
            "          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  \n",
            "          whether there are values you can (re)compute.                                                                 \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   256\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                524,288\n",
            "    Registers Per Thread             register/thread              21\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread     134,217,728\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                            3,276.80\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           10\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.90\n",
            "    Achieved Active Warps Per SM           warp        26.53\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 17.1%                                                                                     \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.9%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- -------------\n",
            "    Metric Name                Metric Unit  Metric Value\n",
            "    -------------------------- ----------- -------------\n",
            "    Average DRAM Active Cycles       cycle 24,894,958.50\n",
            "    Total DRAM Elapsed Cycles        cycle   274,235,392\n",
            "    Average L1 Active Cycles         cycle  4,011,991.27\n",
            "    Total L1 Elapsed Cycles          cycle   160,550,920\n",
            "    Average L2 Active Cycles         cycle  5,852,476.16\n",
            "    Total L2 Elapsed Cycles          cycle   187,738,656\n",
            "    Average SM Active Cycles         cycle  4,011,991.27\n",
            "    Total SM Elapsed Cycles          cycle   160,550,920\n",
            "    Average SMSP Active Cycles       cycle  4,011,941.30\n",
            "    Total SMSP Elapsed Cycles        cycle   642,203,680\n",
            "    -------------------------- ----------- -------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9"
      ],
      "metadata": {
        "id": "rmYCfXChOfv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d_new.cu\n",
        "\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 32\n",
        "#define BLOCK_Y 2\n",
        "#define BLOCK_Z 4\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "\t         \t      const float* __restrict__ d_u1,\n",
        "\t\t\t            float* __restrict__ d_u2)\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  k    = threadIdx.z + blockIdx.z*BLOCK_Z;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  indg = i + j*JOFF + k*KOFF;\n",
        "\n",
        "  if (i>=0 && i<=NX-1 && j>=0 && j<=NY-1 && k>=0 && k<=NZ-1) {\n",
        "    if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "      u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "    }\n",
        "    else {\n",
        "      u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "           + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "           + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "    }\n",
        "    d_u2[indg] = u2;\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=512, NY=512, NZ=512,\n",
        "            REPEAT=20, bx, by, bz, i, j, k;\n",
        "  float    *h_u1, *h_u2, *h_foo,\n",
        "           *d_u1, *d_u2, *d_foo;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // Gold treatment\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    Gold_laplace3d(NX, NY, NZ, h_u1, h_u2);\n",
        "    h_foo = h_u1; h_u1 = h_u2; h_u2 = h_foo;   // swap h_u1 and h_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx Gold_laplace3d: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "  bz = 1 + (NZ-1)/BLOCK_Z;\n",
        "\n",
        "  dim3 dimGrid(bx,by,bz);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y,BLOCK_Z);\n",
        "\n",
        "  // Execute GPU kernel\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "\n",
        "  for (i=0; i<REPEAT; i++) {\n",
        "    cudaEventRecord(start);\n",
        "    GPU_laplace3d<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "    printf(\"iteration %i : mili = %f \\n \\n\",i,milli);\n",
        "    printf(\"transfer rate read + write for iteration %i %f GB/s \\n\\n\",i,(2*bytes)/(1024.0*1024.0*1024.0*milli*0.001));\n",
        "\n",
        "    getLastCudaError(\"GPU_laplace3d execution failed\\n\");\n",
        "\n",
        "    d_foo = d_u1; d_u1 = d_u2; d_u2 = d_foo;   // swap d_u1 and d_u2\n",
        "  }\n",
        "\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"%dx GPU_laplace3d_new: %.1f (ms) \\n\\n\", REPEAT, milli);\n",
        "\n",
        "  // Read back GPU results\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(h_u2, d_u1, bytes, cudaMemcpyDeviceToHost) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u2 to host: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "  // error check\n",
        "\n",
        "  float err = 0.0;\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "        err += (h_u1[ind]-h_u2[ind])*(h_u1[ind]-h_u2[ind]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"rms error = %f \\n\",sqrt(err/ (float)(NX*NY*NZ)));\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8eb71c-e110-4eb5-9899-09b13f7a7a8a",
        "id": "2hORxnEyPZ3R"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace3d_new.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d_new.cu -o laplace3d_new -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44553621-78d2-4d7d-80fa-81ed226a07d1",
        "id": "Dyg4VdklPZ3S"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 21 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cfb130b-e2bd-4982-fddf-3f4f6091b4b8",
        "id": "pQhjeTOCPZ3S"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 114.2 (ms) \n",
            "\n",
            "20x Gold_laplace3d: 27634.8 (ms) \n",
            "\n",
            "iteration 0 : mili = 7.148544 \n",
            " \n",
            "transfer rate read + write for iteration 0 139.888630 GB/s \n",
            "\n",
            "iteration 1 : mili = 6.926368 \n",
            " \n",
            "transfer rate read + write for iteration 1 144.375806 GB/s \n",
            "\n",
            "iteration 2 : mili = 6.925888 \n",
            " \n",
            "transfer rate read + write for iteration 2 144.385816 GB/s \n",
            "\n",
            "iteration 3 : mili = 6.922240 \n",
            " \n",
            "transfer rate read + write for iteration 3 144.461913 GB/s \n",
            "\n",
            "iteration 4 : mili = 6.924480 \n",
            " \n",
            "transfer rate read + write for iteration 4 144.415177 GB/s \n",
            "\n",
            "iteration 5 : mili = 6.922656 \n",
            " \n",
            "transfer rate read + write for iteration 5 144.453226 GB/s \n",
            "\n",
            "iteration 6 : mili = 6.922176 \n",
            " \n",
            "transfer rate read + write for iteration 6 144.463246 GB/s \n",
            "\n",
            "iteration 7 : mili = 6.928352 \n",
            " \n",
            "transfer rate read + write for iteration 7 144.334471 GB/s \n",
            "\n",
            "iteration 8 : mili = 6.927008 \n",
            " \n",
            "transfer rate read + write for iteration 8 144.362469 GB/s \n",
            "\n",
            "iteration 9 : mili = 6.928352 \n",
            " \n",
            "transfer rate read + write for iteration 9 144.334471 GB/s \n",
            "\n",
            "iteration 10 : mili = 6.922496 \n",
            " \n",
            "transfer rate read + write for iteration 10 144.456569 GB/s \n",
            "\n",
            "iteration 11 : mili = 6.921664 \n",
            " \n",
            "transfer rate read + write for iteration 11 144.473925 GB/s \n",
            "\n",
            "iteration 12 : mili = 6.922304 \n",
            " \n",
            "transfer rate read + write for iteration 12 144.460569 GB/s \n",
            "\n",
            "iteration 13 : mili = 6.926432 \n",
            " \n",
            "transfer rate read + write for iteration 13 144.374475 GB/s \n",
            "\n",
            "iteration 14 : mili = 6.920096 \n",
            " \n",
            "transfer rate read + write for iteration 14 144.506667 GB/s \n",
            "\n",
            "iteration 15 : mili = 6.924320 \n",
            " \n",
            "transfer rate read + write for iteration 15 144.418509 GB/s \n",
            "\n",
            "iteration 16 : mili = 6.926336 \n",
            " \n",
            "transfer rate read + write for iteration 16 144.376482 GB/s \n",
            "\n",
            "iteration 17 : mili = 6.922912 \n",
            " \n",
            "transfer rate read + write for iteration 17 144.447883 GB/s \n",
            "\n",
            "iteration 18 : mili = 6.924320 \n",
            " \n",
            "transfer rate read + write for iteration 18 144.418509 GB/s \n",
            "\n",
            "iteration 19 : mili = 6.922240 \n",
            " \n",
            "transfer rate read + write for iteration 19 144.461913 GB/s \n",
            "\n",
            "20x GPU_laplace3d_new: 6.9 (ms) \n",
            "\n",
            "Copy u2 to host: 114.6 (ms) \n",
            "\n",
            "rms error = 0.000000 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Lets try to make a seperate kernel for read and for write </h2>"
      ],
      "metadata": {
        "id": "Rqr3Js8ubrJc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "emjZXGntOiVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile laplace3d_new.cu\n",
        "\n",
        "//\n",
        "// Program to solve Laplace equation on a regular 3D grid\n",
        "//\n",
        "\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "#include <string.h>\n",
        "#include <math.h>\n",
        "\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// define kernel block size\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "#define BLOCK_X 32\n",
        "#define BLOCK_Y 2\n",
        "#define BLOCK_Z 4\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// kernel function\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "__global__ void GPU_laplace3d(long long NX, long long NY, long long NZ,\n",
        "\t         \t      const float* __restrict__ d_u1,\n",
        "\t\t\t            float* __restrict__ d_u2)\n",
        "{\n",
        "  long long i, j, k, indg, IOFF, JOFF, KOFF;\n",
        "  float     u2, sixth=1.0f/6.0f;\n",
        "\n",
        "  //\n",
        "  // define global indices and array offsets\n",
        "  //\n",
        "\n",
        "  i    = threadIdx.x + blockIdx.x*BLOCK_X;\n",
        "  j    = threadIdx.y + blockIdx.y*BLOCK_Y;\n",
        "  k    = threadIdx.z + blockIdx.z*BLOCK_Z;\n",
        "\n",
        "  IOFF = 1;\n",
        "  JOFF = NX;\n",
        "  KOFF = NX*NY;\n",
        "\n",
        "  indg = i + j*JOFF + k*KOFF;\n",
        "\n",
        "  if (i>=0 && i<=NX-1 && j>=0 && j<=NY-1 && k>=0 && k<=NZ-1) {\n",
        "    if (i==0 || i==NX-1 || j==0 || j==NY-1 || k==0 || k==NZ-1) {\n",
        "      u2 = d_u1[indg];  // Dirichlet b.c.'s\n",
        "    }\n",
        "    else {\n",
        "      u2 = ( d_u1[indg-IOFF] + d_u1[indg+IOFF]\n",
        "           + d_u1[indg-JOFF] + d_u1[indg+JOFF]\n",
        "           + d_u1[indg-KOFF] + d_u1[indg+KOFF] ) * sixth;\n",
        "    }\n",
        "    d_u2[indg] = u2;\n",
        "  }\n",
        "}\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Gold routine -- reference C++ code\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "void Gold_laplace3d(long long NX, long long NY, long long NZ, float* u1, float* u2)\n",
        "{\n",
        "  long long i, j, k, ind;\n",
        "  float     sixth=1.0f/6.0f;  // predefining this improves performance more than 10%\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {   // i loop innermost for sequential memory access\n",
        "\t      ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1) {\n",
        "          u2[ind] = u1[ind];          // Dirichlet b.c.'s\n",
        "        }\n",
        "        else {\n",
        "          u2[ind] = ( u1[ind-1    ] + u1[ind+1    ]\n",
        "                    + u1[ind-NX   ] + u1[ind+NX   ]\n",
        "                    + u1[ind-NX*NY] + u1[ind+NX*NY] ) * sixth;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// Read-only kernel\n",
        "__global__ void GPU_read_only(long long NX, long long NY, long long NZ,\n",
        "                                const float* __restrict__ d_u1,\n",
        "                                float* __restrict__ dummy)\n",
        "{\n",
        "    long long i = threadIdx.x + blockIdx.x * BLOCK_X;\n",
        "    long long j = threadIdx.y + blockIdx.y * BLOCK_Y;\n",
        "    long long k = threadIdx.z + blockIdx.z * BLOCK_Z;\n",
        "\n",
        "    if (i < NX && j < NY && k < NZ) {\n",
        "        long long indg = i + j * NX + k * NX * NY;\n",
        "        // Read operation\n",
        "        float val = d_u1[indg];\n",
        "        // Prevent optimization by writing to dummy\n",
        "        dummy[indg] = val;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void GPU_write_only(long long NX, long long NY, long long NZ,\n",
        "                                const float* __restrict__ d_u1,\n",
        "                                float* __restrict__ d_u2)\n",
        "{\n",
        "    long long i = threadIdx.x + blockIdx.x * BLOCK_X;\n",
        "    long long j = threadIdx.y + blockIdx.y * BLOCK_Y;\n",
        "    long long k = threadIdx.z + blockIdx.z * BLOCK_Z;\n",
        "\n",
        "    if (i < NX && j < NY && k < NZ) {\n",
        "        long long indg = i + j * NX + k * NX * NY;\n",
        "\n",
        "        // Perform only write (no read)\n",
        "        d_u2[indg] = d_u1[indg];  // Just writing the value without reading\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "// Main program\n",
        "////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "int main(int argc, const char **argv){\n",
        "\n",
        "  int       NX=512, NY=512, NZ=512,\n",
        "            bx, by, bz, i, j, k;\n",
        "  float    *h_u1, *h_u2,\n",
        "           *d_u1, *d_u2;\n",
        "\n",
        "  size_t    ind, bytes = sizeof(float) * NX*NY*NZ;\n",
        "\n",
        "  printf(\"Grid dimensions: %d x %d x %d \\n\\n\", NX, NY, NZ);\n",
        "\n",
        "  // initialise card\n",
        "\n",
        "  findCudaDevice(argc, argv);\n",
        "\n",
        "  // initialise CUDA timing\n",
        "\n",
        "  float milli;\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // allocate memory for arrays\n",
        "\n",
        "  h_u1 = (float *)malloc(bytes);\n",
        "  h_u2 = (float *)malloc(bytes);\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u1, bytes) );\n",
        "  checkCudaErrors( cudaMalloc((void **)&d_u2, bytes) );\n",
        "\n",
        "  // initialise u1\n",
        "\n",
        "  for (k=0; k<NZ; k++) {\n",
        "    for (j=0; j<NY; j++) {\n",
        "      for (i=0; i<NX; i++) {\n",
        "        ind = i + j*NX + k*NX*NY;\n",
        "\n",
        "        if (i==0 || i==NX-1 || j==0 || j==NY-1|| k==0 || k==NZ-1)\n",
        "          h_u1[ind] = 1.0f;           // Dirichlet b.c.'s\n",
        "        else\n",
        "          h_u1[ind] = 0.0f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // copy u1 to device\n",
        "\n",
        "  cudaEventRecord(start);\n",
        "  checkCudaErrors( cudaMemcpy(d_u1, h_u1, bytes,\n",
        "                              cudaMemcpyHostToDevice) );\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "  printf(\"Copy u1 to device: %.1f (ms) \\n\\n\", milli);\n",
        "\n",
        "\n",
        "  // Set up the execution configuration\n",
        "\n",
        "  bx = 1 + (NX-1)/BLOCK_X;\n",
        "  by = 1 + (NY-1)/BLOCK_Y;\n",
        "  bz = 1 + (NZ-1)/BLOCK_Z;\n",
        "\n",
        "  dim3 dimGrid(bx,by,bz);\n",
        "  dim3 dimBlock(BLOCK_X,BLOCK_Y,BLOCK_Z);\n",
        "\n",
        "  // Timing variables\n",
        "// Execute the read-only kernel and time it\n",
        "  cudaEventRecord(start);\n",
        "  GPU_read_only<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  double time_read = milli;  // Store read execution time\n",
        "  double time_sec = milli * 1e-3;\n",
        "  double read_bytes = (double)bytes; // Total bytes read per iteration\n",
        "  double read_bandwidth = read_bytes / (time_sec * 1e9);\n",
        "  printf(\"Read-only kernel time: %.3f ms, Effective read bandwidth: %.3f GB/s\\n\", milli, read_bandwidth);\n",
        "\n",
        "  // Execute the write-only kernel and time it\n",
        "  cudaEventRecord(start);\n",
        "  GPU_write_only<<<dimGrid, dimBlock>>>(NX, NY, NZ, d_u1, d_u2);\n",
        "  cudaEventRecord(stop);\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaEventElapsedTime(&milli, start, stop);\n",
        "\n",
        "  double time_write = milli;  // Store write execution time\n",
        "  time_sec = milli * 1e-3;\n",
        "  double write_bytes = (double)bytes; // Total bytes written per iteration\n",
        "  double write_bandwidth = write_bytes / (time_sec * 1e9);\n",
        "  printf(\"Write-only kernel time: %.3f ms, Effective write bandwidth: %.3f GB/s\\n\", milli, write_bandwidth);\n",
        "\n",
        "  // Compute overall time (sum of read and write times)\n",
        "  double overall_time = time_read + time_write;\n",
        "\n",
        "  // Compute overall effective bandwidth\n",
        "  double total_bytes = read_bytes + write_bytes; // Total data moved (read + write)\n",
        "  double overall_bandwidth = total_bytes / (overall_time * 1e-3 * 1e9); // Convert ms to s\n",
        "\n",
        "  printf(\"Overall kernel time: %.3f ms\\n\", overall_time);\n",
        "  printf(\"Overall effective bandwidth: %.3f GB/s\\n\", overall_bandwidth);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " // Release GPU and CPU memory\n",
        "\n",
        "  checkCudaErrors( cudaFree(d_u1) );\n",
        "  checkCudaErrors( cudaFree(d_u2) );\n",
        "  free(h_u1);\n",
        "  free(h_u2);\n",
        "\n",
        "  cudaDeviceReset();\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528a897b-2356-4e00-f27e-d420b7629a75",
        "id": "t0rn7Rw7cDvM"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting laplace3d_new.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc laplace3d_new.cu -o laplace3d_new -I. -lineinfo -arch=sm_70 --ptxas-options=-v --use_fast_math -lcudart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee7ad4d-2b20-4164-e91b-a7cec99ac53b",
        "id": "abADJ0egcDvN"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ptxas info    : 0 bytes gmem\n",
            "ptxas info    : Compiling entry function '_Z14GPU_write_onlyxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z14GPU_write_onlyxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 392 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z13GPU_read_onlyxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_read_onlyxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 10 registers, 392 bytes cmem[0]\n",
            "ptxas info    : Compiling entry function '_Z13GPU_laplace3dxxxPKfPf' for 'sm_70'\n",
            "ptxas info    : Function properties for _Z13GPU_laplace3dxxxPKfPf\n",
            "    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n",
            "ptxas info    : Used 21 registers, 392 bytes cmem[0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./laplace3d_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd73f34-2c08-4274-c885-39a1709881c3",
        "id": "bG1h6e6BcDvO"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 512 x 512 x 512 \n",
            "\n",
            "GPU Device 0: \"Turing\" with compute capability 7.5\n",
            "\n",
            "Copy u1 to device: 118.8 (ms) \n",
            "\n",
            "Read-only kernel time: 5.556 ms, Effective read bandwidth: 96.630 GB/s\n",
            "Write-only kernel time: 5.362 ms, Effective write bandwidth: 100.131 GB/s\n",
            "Overall kernel time: 10.918 ms\n",
            "Overall effective bandwidth: 98.350 GB/s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wW0Zz91GbqTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gJOKVTcSbqV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cLUITC2BOiYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unassign"
      ],
      "metadata": {
        "id": "RqLtO19MOir3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "MtAImDxLjEpz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}